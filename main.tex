
%%
%% forked from https://gits-15.sys.kth.se/giampi/kthlatex kthlatex-0.2rc4 on 2020-02-13
%% expanded upon by Gerald Q. Maguire Jr.
%% This template has been adapted by Anders Sjögren to the University
%% Engineering Program in Computer Science at KTH ICT. This adaptation was
%% translation of English headings into Swedish with the addition of Swedish.
%% Many thanks to others who have provided constructive input regarding the template.

% Make it possible to conditionally depend on the TeX engine used
\RequirePackage{ifxetex}
\RequirePackage{ifluatex}
\newif\ifxeorlua
\ifxetex\xeorluatrue\fi
\ifluatex\xeorluatrue\fi

\ifxeorlua
% The following is to ensure that the PDF uses a recent version rather than the typical PDF 1-5
%  This same version of PDF should be set as an option for hyperef

\RequirePackage{expl3}
\ExplSyntaxOn
%pdf_version_gset:n{2.0}
%\pdf_version_gset:n{1.5}

%% Alternatively, if you have a LaTeX newer than June 2022, you can use the following. However, then you have to remove the pdfversion from hyperef. It also breaks hyperxmp. So perhaps it is too early to try using it!
%\DocumentMetadata
%{
%% testphase = phase-I, % tagging without paragraph tagging
% testphase = phase-II % tagging with paragraph tagging and other new stuff.
%pdfversion = 2.0 % pdfversion must be set here.
%}

% Optionally, you can set the uncompress flag to make it easier to examine the PDF
%\pdf_uncompress: % to check the pdf
\ExplSyntaxOff
\else
\RequirePackage{expl3}
\ExplSyntaxOn
%\pdf_version_gset:n{2.0}
\pdf_version_gset:n{1.5}
\ExplSyntaxOff
\fi

%% Define a pair of commands to disable and reenable specific packages - see https://tex.stackexchange.com/questions/39415/unload-a-latex-package
\makeatletter
\newcommand{\disablepackage}[2]{%
  \disable@package@load{#1}{#2}%
}
\newcommand{\reenablepackage}[1]{%
  \reenable@package@load{#1}%
}
\makeatother
%% To avoid the warning: "Package transparent Warning: Loading aborted, because pdfTeX is not running in PDF mode."
\ifxeorlua
\disablepackage{transparent}{}
\fi



%% The template is designed to handle a thesis in English or Swedish
% set the default language to english or swedish by passing an option to the documentclass - this handles the inside title page
% To optimize for digital output (this changes the color palette add the option: digitaloutput
% To use \ifnomenclature add the option nomenclature
% To use bibtex or biblatex - include one of these as an option
\documentclass[nomenclature, english, bibtex]{kththesis}
%\documentclass[swedish, biblatex]{kththesis}
% if pdflatex \usepackage[utf8]{inputenc}


%% Conventions for todo notes:
% Informational
%% \generalExpl{Comments/directions/... in English}
\newcommand*{\generalExpl}[1]{\todo[inline]{#1}}

% Language-specific information (currently in English or Swedish)
\newcommand*{\engExpl}[1]{\todo[inline, backgroundcolor=kth-lightgreen40]{#1}} %% \engExpl{English descriptions about formatting}
\newcommand*{\sweExpl}[1]{\todo[inline, backgroundcolor=kth-lightblue40]{#1}}  %% % \sweExpl{Text på svenska}

% warnings
\newcommand*{\warningExpl}[1]{\todo[inline, backgroundcolor=kth-lightred40]{#1}} %% \warningExpl{warnings}

% Uncomment to hide specific comments, to hide **all** ToDos add `final` to
% document class
% \renewcommand\warningExpl[1]{}
% \renewcommand\generalExpl[1]{}
% \renewcommand\engExpl[1]{}
% For example uncommenting the following line hides the Swedish language explanations
% \renewcommand\sweExpl[1]{}


% \usepackage[style=numeric,sorting=none,backend=biber]{biblatex}
\ifbiblatex
    %\usepackage[language=english,bibstyle=authoryear,citestyle=authoryear, maxbibnames=99]{biblatex}
    % Alternatively you might use another style, such as IEEE and use citestyle=numeric-comp  to put multiple citations in a single pair of square brackets
    \usepackage[style=ieee,citestyle=numeric-comp]{biblatex}
    \addbibresource{references}
    %\DeclareLanguageMapping{norsk}{norwegian}
\else
    % The line(s) below are for BibTeX
    \bibliographystyle{bibstyle/myIEEEtran}
    %\bibliographystyle{apalike}
\fi


% include a variety of packages that are useful
\input{lib/includes}
\input{lib/kthcolors}

%\glsdisablehyper
%\makeglossaries
%\makenoidxglossaries
%\input{lib/acronyms}                %load the acronyms file

\input{lib/defines}  % load some additional definitions to make writing more consistent

% The following is needed in conjunction with generating the DiVA data with abstracts and keywords using the scontents package and a modified listings environment
%\usepackage{listings}   %  already included
\ExplSyntaxOn
\newcommand\typestoredx[2]{\expandafter\__scontents_typestored_internal:nn\expandafter{#1} {#2}}
\ExplSyntaxOff
\makeatletter
\let\verbatimsc\@undefined
\let\endverbatimsc\@undefined
\lst@AddToHook{Init}{\hyphenpenalty=50\relax}
\makeatother


\lstnewenvironment{verbatimsc}
    {
    \lstset{%
        basicstyle=\ttfamily\tiny,
        backgroundcolor=\color{white},
        %basicstyle=\tiny,
        %columns=fullflexible,
        columns=[l]fixed,
        language=[LaTeX]TeX,
        %numbers=left,
        %numberstyle=\tiny\color{gray},
        keywordstyle=\color{red},
        breaklines=true,                 % sets automatic line breaking
        breakatwhitespace=true,          % sets if automatic breaks should only happen at whitespace
        %keepspaces=false,
        breakindent=0em,
        %fancyvrb=true,
        frame=none,                     % turn off any box
        postbreak={}                    % turn off any hook arrow for continuation lines
    }
}{}

%% Add some more keywords to bring out the structure more
\lstdefinestyle{[LaTeX]TeX}{
morekeywords={begin, todo, textbf, textit, texttt}
}

%% definition of new command for bytefield package
\newcommand{\colorbitbox}[3]{%
	\rlap{\bitbox{#2}{\color{#1}\rule{\width}{\height}}}%
	\bitbox{#2}{#3}}




% define a left aligned table cell that is ragged right
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}

% Because backref is not compatible with biblatex
\ifbiblatex
    \usepackage[plainpages=false]{hyperref}
\else
    \usepackage[
    backref=page,
    pagebackref=false,
    plainpages=false,
                            % PDF related options
    unicode=true,           % Unicode encoded PDF strings
    bookmarks=true,         % generate bookmarks in PDF files
    bookmarksopen=false,    % Do not automatically open the bookmarks in the PDF reading program
    pdfpagemode=UseNone,    % None, UseOutlines, UseThumbs, or FullScreen
    destlabel,              % better naming of destinations
    pdfencoding=auto,       % for unicode in
    ]{hyperref}
    \makeatletter
    \ltx@ifpackageloaded{attachfile2}{
    % cannot use backref if one is using attachfile
    }
    {\usepackage{backref}
    %
    % Customize list of backreferences.
    % From https://tex.stackexchange.com/a/183735/1340
    \renewcommand*{\backref}[1]{}
    \renewcommand*{\backrefalt}[4]{%
    \ifcase #1%
          \or [Page~#2.]%
          \else [Pages~#2.]%
    \fi%
    }
    }
    \makeatother

\fi
\usepackage[all]{hypcap}	%% prevents an issue related to hyperref and caption linking

%% Acronyms
% note that nonumberlist - removes the cross references to the pages where the acronym appears
% note that super will set the descriptions text aligned
% note that nomain - does not produce a main glossary, thus only acronyms will be in the glossary
% note that nopostdot - will prevent there being a period at the end of each entry
\usepackage[acronym, style=super, section=section, nonumberlist, nomain,
nopostdot]{glossaries}
\setlength{\glsdescwidth}{0.75\textwidth}
\usepackage[]{glossaries-extra}
\ifinswedish
    %\usepackage{glossaries-swedish}
\fi

%% For use with the README_notes
% Define a new type of glossary so that the acronyms defined in the README_notes document can be distinct from those in the thesis template
% the tlg, tld, and dn will be the file extensions used for this glossary
\newglossary[tlg]{readme}{tld}{tdn}{README acronyms}


\input{lib/includes-after-hyperref}

%\glsdisablehyper
\makeglossaries
%\makenoidxglossaries

% The following bit of ugliness is because of the problems PDFLaTeX has handling a non-breaking hyphen
% unless it is converted to UTF-8 encoding.
% If you do not use such characters in your acronyms, this could be simplified to just include the acronyms file.
\ifxeorlua
\input{lib/acronyms}                %load the acronyms file
\else
\input{lib/acronyms-for-pdflatex}
\fi


% insert the configuration information with author(s), examiner, supervisor(s), ...
\input{custom_configuration}

\title{Chest X-ray Transmission Map Reconstruction}
\subtitle{Constrained Optimization to Invert a Family of Image Processing Algorithms}

% give the alternative title - i.e., if the thesis is in English, then give a Swedish title
\alttitle{Detta är den svenska översättningen av titeln}
\altsubtitle{Detta är den svenska översättningen av undertiteln}
% alternative, if the thesis is in Swedish, then give an English title
%\alttitle{This is the English translation of the title}
%\altsubtitle{This is the English translation of the subtitle}

% Enter the English and Swedish keywords here for use in the PDF metadata _and_ for later use
% following the respective abstract.
% Try to put the words in the same order in both languages to facilitate matching. For example:
\EnglishKeywords{Nonlinear Optimization, Medical Imaging, Digital Image Processing}
\SwedishKeywords{Canvas Lärplattform, Dockerbehållare, Prestandajustering}

%%%%% For the oral presentation
%% Add this information once your examiner has scheduled your oral presentation
\presentationDateAndTimeISO{2022-03-15 13:00}
\presentationLanguage{eng}
\presentationRoom{via Zoom https://kth-se.zoom.us/j/ddddddddddd}
\presentationAddress{Isafjordsgatan 22 (Kistagången 16)}
\presentationCity{Stockholm}

% When there are multiple opponents, separate their names with '\&'
% Opponent's information
\opponentsNames{A. B. Normal \& A. X. E. Normalè}

% Once a thesis is approved by the examiner, add the TRITA number
% The TRITA number for a thesis consists of two parts: a series (unique to each school)
% and the number in the series, which is formatted as the year followed by a colon and
% then a unique series number for the thesis - starting with 1 each year.
\trita{TRITA -- EECS-EX}{2024:0000}

% Put the title, author, and keyword information into the PDF meta information
\input{lib/pdf_related_includes}


% the custom colors and the commands are defined in defines.tex
\hypersetup{
	colorlinks  = true,
	breaklinks  = true,
	linkcolor   = \linkscolor,
	urlcolor    = \urlscolor,
	citecolor   = \refscolor,
	anchorcolor = black
}

\ifnomenclature
% The following lines make the page numbers and equations hyperlinks in the Nomenclature list
\renewcommand*{\pagedeclaration}[1]{\unskip, \dotfill\hyperlink{page.#1}{page\nobreakspace#1}}
% The following does not work correctly, as the name of the cross-reference is incorrect
%\renewcommand*{\eqdeclaration}[1]{, see equation\nobreakspace(\hyperlink{equation.#1}{#1})}

% You can also change the page heading for the nomenclature
\renewcommand{\nomname}{List of Symbols Used}

% You can even add customization text before the list
\renewcommand{\nompreamble}{The following symbols will be later used within the body of the thesis.}
\makenomenclature
\fi

%
% The commands below are to configure JSON listings
%
% format for JSON listings
\colorlet{punct}{red!60!black}
\definecolor{delim}{RGB}{20,105,176}
\definecolor{numb}{RGB}{106, 109, 32}
\definecolor{string}{RGB}{0, 0, 0}

\lstdefinelanguage{json}{
    numbers=none,
    numberstyle=\small,
    frame=none,
    rulecolor=\color{black},
    showspaces=false,
    showtabs=false,
    breaklines=true,
    postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{gray}\hookrightarrow\space}},
    breakatwhitespace=true,
    basicstyle=\ttfamily\small,
    extendedchars=false,
    upquote=true,
    morestring=[b]",
    stringstyle=\color{string},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1}
      {’}{{\char13}}1,
}

\lstdefinelanguage{XML}
{
  basicstyle=\ttfamily\color{blue}\bfseries\small,
  morestring=[b]",
  morestring=[s]{>}{<},
  morecomment=[s]{<?}{?>},
  stringstyle=\color{black},
  identifierstyle=\color{blue},
  keywordstyle=\color{cyan},
  breaklines=true,
  postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{gray}\hookrightarrow\space}},
  breakatwhitespace=true,
  morekeywords={xmlns,version,type}% list your attributes here
}

% If you use both listing and lstlisting environments - the following makes them both use the same counter and the same formatting in the List of Listings
\makeatletter
\AtBeginDocument{\let\c@listing\c@lstlisting}
\AtBeginDocument{\let\l@listing\l@lstlisting}
\makeatother

% Change the heading for the "List of Listings" to simply "Listings"
\renewcommand{\lstlistlistingname}{Listings}

% number each listing within the chapter
\numberwithin{listing}{chapter}

\usepackage{subfiles}

% To have Creative Commons (CC) license and logos use the doclicense package
% Note that the lowercase version of the license has to be used in the modifier
% i.e., one of by, by-nc, by-nd, by-nc-nd, by-sa, by-nc-sa, zero.
% For background see:
% https://www.kb.se/samverkan-och-utveckling/oppen-tillgang-och-bibsamkonsortiet/open-access-and-bibsam-consortium/open-access/creative-commons-faq-for-researchers.html
% https://kib.ki.se/en/publish-analyse/publish-your-article-open-access/open-licence-your-publication-cc
\begin{comment}
\usepackage[
    type={CC},
    %modifier={by-nc-nd},
    %version={4.0},
    modifier={by-nc},
    imagemodifier={-eu-88x31},  % to get Euro symbol rather than Dollar sign
    hyphenation={RaggedRight},
    version={4.0},
    %modifier={zero},
    %version={1.0},
]{doclicense}
\end{comment}

\begin{document}
%\selectlanguage{swedish}
%
\selectlanguage{english}

%%% Set the numbering for the title page to a numbering series not in the preface or body
\pagenumbering{alph}
\kthcover
\clearpage\thispagestyle{empty}\mbox{} % empty back of front cover
\titlepage

% If you do not want to have a bookinfo page, comment out the line saying \bookinfopage and add a \cleardoublepage
% If you want a bookinfo page: you will get a copyright notice, unless you have used the doclicense package in which case you will get a Creative Commons license. To include the doclicense package, uncomment the configuration of this package above and configure it with your choice of license.
\bookinfopage

% Frontmatter includes the abstracts and table-of-contents
\frontmatter
\setcounter{page}{1}
\begin{abstract}
% The first abstract should be in the language of the thesis.
% Abstract fungerar på svenska också.
  \markboth{\abstractname}{}
\begin{scontents}[store-env=lang]
eng
\end{scontents}
%%% The contents of the abstract (between the begin and end of scontents) will be saved in LaTeX format
%%% and output on the page(s) at the end of the thesis with information for DiVA facilitating the correct
%%% entry of the meta data for your thesis.
%%% These page(s) will be removed before the thesis is inserted into DiVA.
% ----- EXPLANATION -----
% \engExpl{All theses at KTH are \textbf{required} to have an abstract in both \textit{English} and \textit{Swedish}.}

% \engExpl{Exchange students may want to include one or more abstracts in the language(s) used in their home institutions to avoid the need to write another thesis when returning to their home institution.}

% \generalExpl{Keep in mind that most of your potential readers are only going to read your \texttt{title} and \texttt{abstract}. This is why the abstract must give them enough information so that they can decide if this document is relevant to them or not. Otherwise, the likely default choice is to ignore the rest of your document.\\
% An abstract should stand on its own, \ie no citations, cross-references to the body of the document, acronyms must be spelled out, \ldots .\\Write this early and revise as necessary. This will help keep you focused on what you are trying to do.}

\begin{scontents}[store-env=abstracts,print-env=true]
% \generalExpl{Enter your abstract here!}
An abstract is (typically) about 250 and 350 words (1/2 A4-page) with the following components:
% key parts of the abstract
\begin{itemize}
  \item What is the topic area? (optional) Introduces the subject area for the project.
  \item Short problem statement
  \item Why was this problem worth a Bachelor's/Master’s thesis project? (\ie, why is the problem both significant and of a suitable degree of difficulty for a Bachelor's/Master’s thesis project? Why has no one else solved it yet?)
  \item How did you solve the problem? What was your method/insight?
  \item Results/Conclusions/Consequences/Impact: What are your key results/\linebreak[4]conclusions? What will others do based on your results? What can be done now that you have finished - that could not be done before your thesis project was completed?
\end{itemize}

\end{scontents}
% \engExpl{The following are some notes about what can be included (in terms of LaTeX) in your abstract.}
Choice of typeface with \textbackslash textit, \textbackslash textbf, and \textbackslash texttt:  \textit{x}, \textbf{x}, and \texttt{x}.

Text superscripts and subscripts with \textbackslash textsubscript and \textbackslash textsuperscript: A\textsubscript{x} and A\textsuperscript{x}.

Some symbols that you might find useful are available, such as: \textbackslash textregistered, \textbackslash texttrademark, and \textbackslash textcopyright. For example,
the copyright symbol: \textbackslash textcopyright Maguire 2022 results in \textcopyright Maguire 2022. Additionally, here are some examples of text superscripts (which can be combined with some symbols): \textbackslash textsuperscript\{99m\}Tc, A\textbackslash textsuperscript\{*\}, A\textbackslash textsuperscript\{\textbackslash textregistered\}, and A\textbackslash texttrademark resulting in \textsuperscript{99m}Tc, A\textsuperscript{*}, A\textsuperscript{\textregistered}, and A\texttrademark. Two examples of subscripts are: H\textbackslash textsubscript\{2\}O and CO\textbackslash textsubscript\{2\} which produce  H\textsubscript{2}O and CO\textsubscript{2}.

You can use simple environments with begin and end: itemize and enumerate and within these use instances of \textbackslash item.

The following commands can be used: \textbackslash eg, \textbackslash Eg, \textbackslash ie, \textbackslash Ie, \textbackslash etc, and \textbackslash etal: \eg, \Eg, \ie, \Ie, \etc, and \etal.

The following commands for numbering with lowercase Roman numerals: \textbackslash first, \textbackslash Second, \textbackslash third, \textbackslash fourth, \textbackslash fifth, \textbackslash sixth, \textbackslash seventh, and \textbackslash eighth: \first, \Second, \third, \fourth, \fifth, \sixth, \seventh, and \eighth. Note that the second case is set with a capital 'S' to avoid conflicts with the use of second of as a unit in the \texttt{siunitx} package.

Equations using \textbackslash( xxxx \textbackslash) or \textbackslash[ xxxx \textbackslash] can be used in the abstract. For example: \( (C_5O_2H_8)_n \)
or \[ \int_{a}^{b} x^2 \,dx \]
Note that you \textbf{cannot} use an equation between dollar signs.


Even LaTeX comments can be handled by using a backslash to quote the percent symbol, for example: \% comment.
Note that one can include percentages, such as: 51\% or \SI{51}{\percent}.

\subsection*{Keywords}
\begin{scontents}[store-env=keywords,print-env=true]
% If you set the EnglishKeywords earlier, you can retrieve them with:
\InsertKeywords{english}
% If you did not set the EnglishKeywords earlier then simply enter the keywords here:
% comma separate keywords, such as: Canvas Learning Management System, Docker containers, Performance tuning
\end{scontents}
\engExpl{\textbf{Choosing good keywords can help others to locate your paper, thesis, dissertation, \ldots and related work.}}
Choose the most specific keyword from those used in your domain, see for example: the ACM Computing Classification System ({\small \url{https://www.acm.org/publications/computing-classification-system/how-to-use})},
the IEEE Taxonomy ({\small \url{https://www.ieee.org/publications/services/thesaurus-thank-you.html}}), PhySH (Physics Subject Headings)\linebreak[4] ({\small \url{https://physh.aps.org/}}), \ldots or keyword selection tools such as the  National Library of Medicine's Medical Subject Headings (MeSH)  ({\small \url{https://www.nlm.nih.gov/mesh/authors.html}}) or Google's Keyword Tool ({\small \url{https://keywordtool.io/}})\\

\textbf{Formatting the keywords}:
\begin{itemize}
  \item The first letter of a keyword should be set with a capital letter and proper names should be capitalized as usual.
  \item Spell out acronyms and abbreviations.
  \item Avoid "stop words" - as they generally carry little or no information.
  \item List your keywords separated by commas (``,'').
\end{itemize}
Since you should have both English and Swedish keywords - you might think of ordering the keywords in corresponding order (\ie, so that the n\textsuperscript{th} word in each list correspond) - this makes it easier to mechanically find matching keywords.
\end{abstract}
\cleardoublepage
\babelpolyLangStart{swedish}
\begin{abstract}
    \markboth{\abstractname}{}
\begin{scontents}[store-env=lang]
swe
\end{scontents}
% \warningExpl{Inside the following scontents environment, you cannot use a \textbackslash include{filename} as the command rather than the file contents will end up in the for DiVA information. Additionally, you should not use a straight double quote character in the abstracts or keywords, use two single quote characters instead.}
\begin{scontents}[store-env=abstracts,print-env=true]
\generalExpl{Enter your Swedish abstract or summary here!}
% \sweExpl{Alla avhandlingar vid KTH \textbf{måste ha} ett abstrakt på både \textit{engelska} och \textit{svenska}.\\
% Om du skriver din avhandling på svenska ska detta göras först (och placera det som det första abstraktet) - och du bör revidera det vid behov.}

% ----- EXPLANATION -----
% \engExpl{If you are writing your thesis in English, you can leave this until the draft version that goes to your opponent for the written opposition. In this way, you can provide the English and Swedish abstract/summary information that can be used in the announcement for your oral presentation.\\If you are writing your thesis in English, then this section can be a summary targeted at a more general reader. However, if you are writing your thesis in Swedish, then the reverse is true – your abstract should be for your target audience, while an English summary can be written targeted at a more general audience.\\This means that the English abstract and Swedish sammnfattning
% or Swedish abstract and English summary need not be literal translations of each other.}

% \warningExpl{Do not use the \textbackslash glspl\{\} command in an abstract that is not in English, as my programs do not know how to generate plurals in other languages. Instead, you will need to spell these terms out or give the proper plural form. In fact, it is a good idea not to use the glossary commands at all in an abstract/summary in a language other than the language used in the \texttt{acronyms.tex file} - since the glossary package does \textbf{not} support use of more than one language.}

% \engExpl{The abstract in the language used for the thesis should be the first abstract, while the Summary/Sammanfattning in the other language can follow}
\end{scontents}
\subsection*{Nyckelord}
\begin{scontents}[store-env=keywords,print-env=true]
% SwedishKeywords were set earlier, hence we can use alternative 2
\InsertKeywords{swedish}
\end{scontents}
\sweExpl{Nyckelord som beskriver innehållet i uppsatsen eller rapporten}
\end{abstract}
\babelpolyLangStop{swedish}

\cleardoublepage
% ----- EXPLANATION -----
% \generalExpl{If you are an exchange student, use the relevant language or languages for abstracts for your home university, as this will often avoid the need for writing another thesis for your home university.\\
% If you are fluent in other languages, feel free to add the abstracts in one or more of them.}
% \engExpl{Note that you may need to augment the set of languages used in \texttt{polyglossia} or
% \texttt{babel} (see the file \texttt{kththesis.cls}). The following languages include those languages that were used in theses at KTH in 2018-2019, except for one in Chinese.\\
% Remove those versions of abstracts that you do not need.\\
% If you add a new language, when specifying the language for the abstract, use the three-letter ISO 639-2 Code – specifically the "B" (bibliographic) variant of these codes (note that this is the same language code used in DiVA).}

% \babelpolyLangStart{spanish}
% \begin{abstract}
%     \markboth{\abstractname}{}
% \begin{scontents}[store-env=lang]
% spa
% \end{scontents}
% \begin{scontents}[store-env=abstracts,print-env=true]
% Résumé en espagnol.
% \end{scontents}
% \subsection*{Palabras claves}
% \begin{scontents}[store-env=keywords,print-env=true]
% 5-6 Palabras claves
% \end{scontents}
% \end{abstract}
% \babelpolyLangStop{spanish}
% \cleardoublepage

\section*{Acknowledgments}
\markboth{Acknowledgments}{}

% ----- EXPLANATION -----
% \sweExpl{Författarnas tack}

% \engExpl{It is nice to acknowledge the people that have helped you. It is
%   also necessary to acknowledge any special permissions that you have gotten –
%   for example, getting permission from the copyright owner to reproduce a
%   figure. In this case, you should acknowledge them and this permission here
%   and in the figure’s caption. \\
%   Note: If you do \textbf{not} have the copyright owner’s permission, then you \textbf{cannot} use any copyrighted figures/tables/\ldots . Unless stated otherwise all figures/tables/\ldots are generally copyrighted.
% }
% \sweExpl{I detta kapitel kan du ev nämna något om
%   din bakgrund om det påverkar rapporten på något sätt. Har du t ex inte
%   möjlighet att skriva perfekt svenska för att du är nyanländ till landet kan
%   det vara på sin plats att nämna detta här. OBS, detta får dock inte vara en
%   ursäkt för att lämna in en rapport med undermåligt språk, undermålig grammatik och
%   stavning (t ex får fel som en automatisk stavningskontroll och
%   grammatikkontroll kan upptäcka inte förekomma)\\
% En dualism som måste hanteras i hela rapporten och projektet
% }

I would like to thank xxxx for having yyyy. Or in the case of two authors:\\
We would like to thank xxxx for having yyyy.

\acknowlegmentssignature

\fancypagestyle{plain}{}
\renewcommand{\chaptermark}[1]{ \markboth{#1}{}}
\tableofcontents
  \markboth{\contentsname}{}

\cleardoublepage
\listoffigures

\cleardoublepage

\listoftables
\cleardoublepage
\lstlistoflistings\engExpl{If you have listings in your thesis. If not, then remove this preface page.}
\cleardoublepage
% Align the text expansion of the glossary entries
\newglossarystyle{mylong}{%
  \setglossarystyle{long}%
  \renewenvironment{theglossary}%
     {\begin{longtable}[l]{@{}p{\dimexpr 2cm-\tabcolsep}p{0.8\hsize}}}% <-- change the value here
     {\end{longtable}}%
 }
%\glsaddall
%\printglossaries[type=\acronymtype, title={List of acronyms}]
\printglossary[style=mylong, type=\acronymtype, title={List of acronyms and abbreviations}]
%\printglossary[type=\acronymtype, title={List of acronyms and abbreviations}]

%\printnoidxglossary[style=mylong, title={List of acronyms and abbreviations}]
\engExpl{The list of acronyms and abbreviations should be in alphabetical order based on the spelling of the acronym or abbreviation.
}

% if the nomenclature option was specified, then include the nomenclature page(s)
\ifnomenclature
    \cleardoublepage
    % Output the nomenclature list
    \printnomenclature
\fi

%% The following label is essential to know the page number of the last page of the preface
%% It is used to compute the data for the "For DIVA" pages
\label{pg:lastPageofPreface}
% Mainmatter is where the actual contents of the thesis goes
\mainmatter
\glsresetall
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\selectlanguage{english}
\chapter{Introduction}

\section{Background}
Digital radiography systems are ubiquitous in modern healthcare but rely on expensive flat panel detectors. This project supports an alternative approach using scintillator-coupled computer vision cameras, where a small camera sensor images a chest-sized scintillator screen. While this approach could significantly reduce hardware costs, the raw images acquired with this camera system need more sophisticated software processing to reach diagnostic image quality \cite{prokop2003principles}.

As articulated in the project description, the challenge lies in the fact that equipment manufacturers (e.g., Siemens, Philips, GE) apply proprietary post-processing algorithms that include non-linear operations—likely including a combination of denoising, frequency filtering, and dynamic range adjustments. These operations form a "black-box" that varies across manufacturers represented in large image datasets like CheXpert.

\section{Problem Statement}
\subsection{Original Problem Definition}
The core problem is to build an open-source X-ray detector that is cost-effective yet diagnostically useful. However, such systems typically have higher noise, distortions, and sensitivity issues compared to commercial systems. Existing diagnostic models trained on high-quality datasets like CheXpert may not perform adequately on images from these simpler systems.

\subsection{Scientific and Engineering Challenges}
The inverse problem we face has several critical challenges:
\begin{itemize}
    \item \textbf{Ill-posedness}: A single processed image can map back to multiple possible "realistic" transmission maps, making the inverse problem fundamentally ill-posed.
    \item \textbf{Diversity of processing approaches}: Each manufacturer implements proprietary image processing pipelines that create different visual characteristics.
    \item \textbf{Limited ground truth}: We lack labeled data showing the original transmission maps corresponding to processed images.
    \item \textbf{Scaling requirements}: We need to process many images efficiently to build a representative dataset.
\end{itemize}

\section{Purpose}
This research aims to develop accessible X-ray detector technology for resource-limited settings. By creating an open-source computational pipeline that simulates how camera-based X-ray detectors capture chest radiographs, we can:
\begin{itemize}
    \item Enable more affordable diagnostic imaging in remote populations
    \item Provide a common baseline for comparing different detector technologies
    \item Support development of machine learning models specifically trained for low-cost imaging systems
\end{itemize}

\section{Goals}
The specific goals of this research are to:
\begin{enumerate}
    \item Identify properties shared by real X-ray transmission maps and define characteristics that make them "realistic"
    \item Characterize the diversity of image processing pipelines applied to these maps in commercial systems
    \item Develop and evaluate optimization algorithms that can recover plausible transmission maps from processed images
    \item Create a framework that produces realistic simulated images for different detector configurations
\end{enumerate}

\section{Research Methodology}
Our approach involves:
\begin{itemize}
    \item Investigating established image processing algorithms used in radiography
    \item Identifying common properties in processed images and their corresponding transmission maps
    \item Developing optimization algorithms with appropriate forward operators and loss functions
    \item Evaluating recovered maps through comparison with known physics models and visual assessment
\end{itemize}

\section{Thesis Structure}
The remainder of this thesis is organized as follows: Chapter 2 presents background information on X-ray transmission, digital image processing, and relevant optimization methods. Chapter 3 describes our proposed method for solving the inverse problem. Chapter 4 details the implementation of our computational pipeline. Chapter 5 presents experimental results, and Chapter 6 discusses implications and directions for future work.

\chapter{Background}

\section{X-ray Transmission}
\subsection{Beer's Law}
X-ray transmission through matter follows Beer's Law, which describes how the intensity of radiation decreases as it passes through a material. For an X-ray beam passing through a homogeneous material, the intensity $I$ after passing through a thickness $x$ is given by:

\begin{equation}
I = I_0 e^{-\mu x}
\end{equation}

where $I_0$ is the incident intensity and $\mu$ is the linear attenuation coefficient of the material \cite[p.~35]{cullity2014elements}.

For heterogeneous materials like the human body, the equation becomes an integral along the beam path:

\begin{equation}
I(u,v) = I_0(u,v) \cdot \exp\left(-\int_L \mu(s)ds\right)
\end{equation}

where $(u,v)$ represents the detector coordinates, and the integral is taken along the linear path $L$ through the body.

\subsection{Body Tissues and X-ray Attenuation}
For the human body specifically, the relation between incident intensity $I_0$ and transmitted intensity $I_1$ can be modeled as:

\begin{equation}
I_1(x, y) = I_0(x, y) e^{-\mu_s d_s(x,y)} e^{-\mu_b d_b(x,y)} e^{-\mu_l d_l(x, y)}
\end{equation}

where $\mu$ and $d$ represent the attenuation coefficients and tissue thickness over a projected point $(x,y)$ for bone ($b$), lungs ($l$), and other soft tissue ($s$). This relationship forms the physical basis for interpreting chest radiographs, as different tissues create characteristic attenuation patterns \cite{prokop2003principles}.

The beam quality also changes as it passes through tissue—a phenomenon known as "beam hardening," where lower energy photons are preferentially absorbed, resulting in objects closer to the detector having less contrast than objects further away \cite{bushberg2011essential}.

\subsection{Transmission Map Definition}
The transmission map is defined as the ratio of transmitted to incident intensity at each point, essentially capturing the exponential term from Beer's Law:

\begin{equation}
T(x,y) = \frac{I_1(x,y)}{I_0(x,y)} = e^{-\int \mu(x,y,z) dz}
\end{equation}

This map has values between 0 and 1, where 0 represents complete absorption and 1 represents no absorption. The transmission map is the fundamental input to any digital radiography processing pipeline, and recovering it is essential for simulating different detector configurations \cite{seibert2006flat}.

\section{Digital Image Processing in Radiography}
\subsection{Gradation Curves}
Digital radiography systems apply various processing steps to the raw transmission map to produce diagnostically useful images. As described by \cite{prokop2003principles}, gradation curves map detector signals to display values. These include:

\begin{itemize}
    \item \textbf{Automated Signal Normalization}: Detects the collimated region and stretches the relevant histogram range to fit the available gray scale values. This process includes:
    \begin{enumerate}
        \item Detection of the collimated region to exclude direct, unattenuated radiation
        \item Histogram analysis of pixel values within the collimated region
        \item Linear mapping to exclude input values where the histogram is (close to) zero
    \end{enumerate}

    \item \textbf{Gradational Adjustment}: Applies lookup tables to optimize contrast for different anatomical regions. Theoretical optimums like the Kanamori curve \cite{kanamori1966determination} have been proposed, but many radiologists prefer sigmoid-shaped curves that more closely match the appearance of conventional radiographs.
\end{itemize}

\subsection{Spatial Frequency Processing}
\subsubsection{Unsharp Masking}
Unsharp masking enhances edges and fine details in radiographs. The process involves:
\begin{enumerate}
    \item Creating a blurred version of the image using low-pass filtering
    \item Subtracting this from the original to obtain high-frequency information
    \item Combining the original with the weighted high-frequency image
\end{enumerate}

The resulting processed image can be expressed as:
\begin{equation}
I' = I + f \cdot (I - B)
\end{equation}

where $I$ is the original image, $B$ is the blurred version, and $f$ is an enhancement factor, typically around 0.5 for chest radiographs \cite{prokop2003principles}.

Mathematically equivalent formulations include:
\begin{equation}
I' = I - f^* \cdot B
\end{equation}
where $f^* = \frac{f}{1+f}$, and:
\begin{equation}
I' = B + (1+f) \cdot (I - B)
\end{equation}

The effects of unsharp masking depend critically on the size of the filter kernel used to create the blurred image $B$. Kernels of 20-30mm typically give the best results for general chest imaging \cite{prokop1993improved}, as they enhance structures of diagnostic interest while avoiding suppression of relevant pathology.

\subsubsection{Multiscale Processing}
More advanced systems use multiscale decomposition, dividing the image into frequency bands that can be independently processed. As detailed by \cite{stahl2000digital}, this approach allows for:
\begin{itemize}
    \item Size-independent enhancement of structures
    \item Better handling of dynamic range differences
    \item More effective noise suppression
\end{itemize}

The Laplacian pyramid decomposition, introduced by \cite{burt1983laplacian}, is a hierarchical structure where the original image is repeatedly filtered and downsampled:

\begin{enumerate}
    \item The image is blurred with a low-pass filter (typically a 3×3 or 5×5 binomial kernel)
    \item The blurred image is downsampled by a factor of 2 in each direction
    \item The downsampled image is expanded back to the original size using interpolation
    \item The expanded image is subtracted from the original to create a "detail" or bandpass image
    \item The process repeats with the downsampled image as the new input
\end{enumerate}

This creates a series of bandpass images $\{B_0, B_1, ..., B_{n-2}\}$ plus a residual low-pass image $B_{n-1}$. The original image can be perfectly reconstructed by reversing the process:

\begin{equation}
I_{\text{original}} = B_{n-1} + \sum_{i=0}^{n-2} B_i
\end{equation}

For enhancement, each bandpass image can be processed independently before reconstruction. This allows for targeted enhancement of structures of specific sizes without affecting others, as implemented in commercial systems like MUSICA (Agfa) \cite{vuylsteke1994multiscale}, MFP (Fuji) \cite{ogoda1997unsharp}, and UNIQUE (Philips) \cite{stahl2000digital}.

\subsubsection{Dynamic Range Reduction}
Chest radiographs must cope with large attenuation differences between the lungs and mediastinum. Dynamic range reduction (DRR) or compression (DRC) harmonizes the image by reducing these differences while preserving local contrast \cite{cowen1988computer,prokop2003principles}.

With unsharp masking, DRR is accomplished by nonlinear subtraction of a low-pass image with a large kernel (2-4cm). The subtraction weight varies with pixel values—minimal for lungs but substantial (e.g., 0.2) for the mediastinum—effectively increasing density in high-absorption areas without affecting well-exposed regions.

In multiscale processing, DRR works by applying nonlinear transformations to the lowest frequency bands, which contain the large-area density differences \cite{stahl2000digital}.

\subsection{Noise Characteristics and Management}
Noise in radiographic images comes from multiple sources:
\begin{itemize}
    \item Quantum noise (Poisson-distributed, proportional to $\sqrt{N}$ where $N$ is the number of photons)
    \item Detector structure noise
    \item Electronic noise
    \item Quantization noise
\end{itemize}

Noise power is distributed across spatial frequencies and varies with detector dose. After logarithmic processing and intensity inversion in storage phosphor systems, noise is most prominent in high-brightness (low optical density) regions \cite{hillen1987imaging,ogawa1995quantitative}.

Subjectively, noise is most visible in areas with minimal texture or "activity." \cite{stahl2000digital} developed a noise containment approach where enhancement is reduced in noise-sensitive regions defined by spatial frequency, local image density, and local activity measures.

Their blending technique can be expressed as:
\begin{equation}
S_{i, \text{robust}}(x,y) = b_D(x,y) \cdot b_A(x,y) \cdot S_{i, \text{fully\_enh}}(x,y) + (1 - b_D(x,y) \cdot b_A(x,y)) \cdot S_{i, \text{orig}}(x,y)
\end{equation}

where $b_D$ and $b_A$ are weighting factors dependent on local density and activity, respectively.

\section{Nonlinear Optimization}
\subsection{Gradient Descent Methods}
Our approach to recovering transmission maps relies on gradient-based optimization. Given a forward model $F$ that maps transmission maps to processed images, we seek to find the transmission map $x$ that minimizes:

\begin{equation}
\arg\min_{x, \theta} \mathcal{L}(F(x, \theta), y)
\end{equation}

where $y$ is the observed processed image, $\theta$ represents parameters of the forward model, and $\mathcal{L}$ is a suitable loss function.

The optimization uses automatic differentiation to compute gradients efficiently. We employ the Adam optimizer \cite{kingma2014adam}, which adapts the learning rate for each parameter based on estimates of first and second moments of the gradients:

\begin{align}
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
v_t &= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\
\hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \\
\hat{v}_t &= \frac{v_t}{1 - \beta_2^t} \\
\theta_t &= \theta_{t-1} - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{align}

where $g_t$ is the gradient, $m_t$ and $v_t$ are the biased first and second moment estimates, $\hat{m}_t$ and $\hat{v}_t$ are the bias-corrected estimates, and $\alpha$, $\beta_1$, $\beta_2$, $\epsilon$ are hyperparameters.

\subsection{Regularization for Ill-Posed Problems}
\subsubsection{Theory of Regularization}
The inverse problem of finding $x$ such that $F(x) = y$ is ill-posed when:
\begin{itemize}
    \item A solution may not exist for all $y$
    \item Solutions may not be unique
    \item Solutions may not depend continuously on the data (small changes in $y$ can cause large changes in $x$)
\end{itemize}

Tikhonov regularization \cite{tikhonov1977solutions} addresses ill-posedness by adding a penalty term to the objective function:

\begin{equation}
\arg\min_x \{\|F(x) - y\|^2 + \lambda \|Rx\|^2\}
\end{equation}

where $R$ is a regularization operator and $\lambda > 0$ controls the regularization strength.

\subsubsection{Total Variation Regularization}
Total Variation (TV) regularization, introduced by \cite{rudin1992nonlinear}, penalizes the total variation of the solution:

\begin{equation}
TV(x) = \sum_{i,j} \sqrt{(x_{i+1,j} - x_{i,j})^2 + (x_{i,j+1} - x_{i,j})^2}
\end{equation}

This promotes piecewise smoothness while preserving edges, making it particularly suitable for images \cite{chambolle2004algorithm}. The regularized objective becomes:

\begin{equation}
\arg\min_x \{\|F(x) - y\|^2 + \lambda TV(x)\}
\end{equation}

In our JAX implementation, we compute TV as:
\begin{equation}
TV(x) = \frac{1}{2}\left(\|\nabla_x x\|_1 + \|\nabla_y x\|_1\right)
\end{equation}

where $\nabla_x$ and $\nabla_y$ are the horizontal and vertical finite difference operators, implemented using the `jnp.diff` function:

\begin{align}
\|\nabla_x x\|_1 &= \text{mean}(|\text{diff}(x, \text{axis}=0)|^2) \\
\|\nabla_y x\|_1 &= \text{mean}(|\text{diff}(x, \text{axis}=1)|^2)
\end{align}

TV regularization is particularly effective for the transmission map reconstruction problem because:
\begin{itemize}
    \item Transmission maps are expected to be piecewise smooth, with discontinuities occurring at tissue boundaries
    \item TV preserves these important boundaries while suppressing noise
    \item It does not over-smooth the image, unlike Gaussian or L2 regularization
\end{itemize}

\subsubsection{Physics-Based Regularization}
Beyond generic smoothness priors like TV, physics-based constraints can significantly improve reconstruction. These include:
\begin{itemize}
    \item \textbf{Value range constraints}: Transmission values must be between 0 and 1
    \item \textbf{Anatomical constraints}: Different tissues have characteristic ranges of attenuation
    \item \textbf{Segment-specific behavior}: Enforcing consistent properties within anatomical segments
\end{itemize}

Projection operators enforce these constraints during optimization:
\begin{equation}
x_{t+1} = P(x_t - \alpha \nabla f(x_t))
\end{equation}

where $P$ is a projection operator that maps to the feasible set. For example, the hypercube projection implemented in Optax \cite{optax2020github} ensures values remain within valid bounds:
\begin{equation}
P_{[a,b]}(x) = \max(a, \min(b, x))
\end{equation}


\section{Image Quality Metrics}
\subsection{Mean Squared Error (MSE)}
The mean squared error quantifies the average squared difference between pixel values:
\begin{equation}
\text{MSE}(x, y) = \frac{1}{N} \sum_{i=1}^N (x_i - y_i)^2
\end{equation}

While simple to compute, MSE does not always correlate well with perceived image quality.

\subsection{Structural Similarity Index (SSIM)}
SSIM \cite{wang2004image} better aligns with human perception by considering structural information:
\begin{equation}
\text{SSIM}(x, y) = \frac{(2\mu_x\mu_y + C_1)(2\sigma_{xy} + C_2)}{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)}
\end{equation}

where $\mu_x$, $\mu_y$ are means, $\sigma_x$, $\sigma_y$ are standard deviations, $\sigma_{xy}$ is the covariance, and $C_1$, $C_2$ are constants to avoid instability.

\subsection{Peak Signal-to-Noise Ratio (PSNR)}
PSNR measures the ratio between the maximum possible power of a signal and the power of corrupting noise:
\begin{equation}
\text{PSNR}(x, y) = 10 \cdot \log_{10}\left(\frac{\text{MAX}_I^2}{\text{MSE}(x, y)}\right)
\end{equation}

where $\text{MAX}_I$ is the maximum possible pixel value.

\section{Available Chest X-ray Datasets}
Several large datasets of chest radiographs are available, though none provide the raw transmission maps:
\begin{itemize}
    \item \textbf{CheXpert} \cite{irvin2019chexpert}: Over 200,000 images from Stanford Hospital, with labels for 14 different pathologies
    \item \textbf{MIMIC-CXR} \cite{johnson2019mimic}: Over 377,000 chest radiographs from Beth Israel Deaconess Medical Center
    \item \textbf{Open-i} \cite{demner2016preparing}: A collection from the National Library of Medicine with associated radiology reports

    \item \textbf{PLCO} \cite{gohagan2000prostate}: From the Prostate, Lung, Colorectal and Ovarian Cancer Screening Trial
\end{itemize}

None of these datasets report radiography hardware specifications or processing parameters, making the inverse problem challenging.

\section{Related Work}
\subsection{Digital Radiography Processing}
Digital radiography processing has evolved from simple unsharp masking to sophisticated multiscale techniques. \cite{prokop2003principles} provides a comprehensive overview of the principles used in clinical systems, while \cite{stahl2000digital} details the nonlinear multiscale approach now common in commercial systems.

\subsection{Inverse Problems in Medical Imaging}
While image recovery from processed versions has been studied in photography \cite{moran2020deeptone}, similar work in medical imaging is limited. Most related work focuses on denoising, deblurring, or super-resolution rather than recovering physically meaningful quantities like transmission maps \cite{jin2017deep}.

\subsection{Low-Cost X-ray Imaging}
Several initiatives aim to develop affordable X-ray systems for resource-limited settings. The World Health Organization's specifications for basic radiographic systems \cite{who2014specifications} guide such efforts. \cite{mollura2010radiology} highlights the critical need for affordable diagnostic imaging in developing countries.

\section{Summary}
The background presented establishes the foundation for our approach to transmission map recovery. We understand the physics of X-ray transmission, the typical processing steps applied in digital radiography, and the mathematical framework for tackling the inverse problem. Our approach leverages advanced optimization techniques, appropriate regularization, and insights from the literature on digital radiography processing to recover physically plausible transmission maps from processed images.

\chapter{Method}

\section{Mathematical Model}
\label{sec:mathematicalModel}

For this thesis project, a family of models \todo{is this the right term} are explored to recover transmission maps under the lack of prior knowledge about the ground truth, and the
diversity of black-box models that are used in practice to process radiographic images. The motivations \todo{can we say this?} driving the design of the methods can be summarized as follows:

\begin{itemize}
    \item If all images are processed to match a same visual profile \todo{cite DIP paper or reference to section}, we could model all black boxes using a single,
        although parameterized, image processing algorithm.
    \item If we recover an appropriate forward model (image processing algorithm) \todo{express forward model and image processing are synonims}, we can reconstruct a
        'realistic' transmission map from the processed images.
\end{itemize}

Given the previous knowledge, we can formulate a general optimization problem using the following notation:

\begin{itemize}
    \item $N$ as the size of the image dataset.
    \item $d$ is the number of parameters of the image processing operator.
    \item $r, c$ as the fixed row and columns in pixels, respectively, of all images in the dataset.
    \item $Y = \{ y_1, y_2, \dots, y_N \}, y_i \in [0, 1]^{r, c}$ is the given set of processed X-ray images.
    \item $X=\{x_{1}, x_{2}, \dots, x_{N}\} $ as the corresponding set of unknown transmission maps, with $x_i \in [0,1]^{r,c}$ for all $i \in \{1,2,\dots,N\}$. \todo{Image formal notation?}
    \item $F:[0,1]^{r, c} \times \mathbb{R}^d \to [0, 1]^{r, c}$ \todo{can we specify domain sets?} as a known image processing algorithm with parameters $\theta \in \mathbb{R}^d$.
\end{itemize}

Then, we formulate the optimization problem to recover $X$ and $\theta$

\begin{equation}
    \min_{X, \theta} \frac{1}{N}\sum_{i=1}^{N} \ell(y_i, F(x_i, \theta)) + \lambda R(X)
\end{equation}

where $\ell$ is a loss function measuring the discrepancy between the processed versions of our estimated transmission
maps and the observed processed images, $R$ is a regularization term applied to $X$, with weights $\lambda$.

Furthermore, the following concrete models can be considered:

\begin{description}
    \item[Known parameters] If we assume there is a known set of parameters for the operator $F$ that can model all the observed processed images, then we get
    \begin{equation}
        \min_{X} \frac{1}{N}\sum_{i=1}^{N} \ell(y_i, F(x_i, \theta)) + \lambda R(X)
    \end{equation}
    such that $\theta \in \mathbb{R}^d$ is known.
    \item[Common operator parameters] If we assume that there exists a single set of parameters $\theta$ (yet unknown) for all processed
    images, i.e. $F(x_i, \theta) = y_i$ for all $i = \{1,\dots,N\}$. Then the problem can be modeled as
    \begin{equation}
        \min_{X, \theta} \frac{1}{N}\sum_{i=1}^{N} \ell(y_i, F(x_i, \theta)) + \lambda R(X).
    \end{equation}
    \item[Per-image operator parameters] If we assume we can model all the observed processed images with $F$, but not necessarily with the same parameters on an image bases,
    i.e. $F(x_i, \theta_i) = y_i$ for all $i = \{1,\dots,N\}$, with  $\theta = \{ \theta_1, \theta_2, \dots, \theta_N \}$. Then we aim to solve
    \begin{equation}
        \min_{X, \theta} \frac{1}{N}\sum_{i=1}^{N} \ell(y_i, F(x_i, \theta_i)) + \lambda R(X),
    \end{equation}
    where we now have a set of parameters $\theta = \{\theta_1, \theta_2, \dots, \theta_N\} \text{ with }\theta_i \in \mathbb{R}^{d}$.
\end{description}


The method employed in this project involved an iterative process of defining a loss function and regularization,
which can be applied to all described model variations, and finding a framework that can yield results considered realistic.
In the following sections, we will discuss the choices made for the functions $F$, $\ell$ and $R$, and how these contribute
to the reconstruction of transmission maps.

\section{Data Collection and Preprocessing}
\label{sec:dataCollection}


\subsection{Transmission Maps}

To our knowledge, there are currently no open datasets of X-ray transmission maps.
We had access to, precisely, two images taken from the same chest phantom at the Hard X-ray lab in KTH, one paired with a processed
image produced by a clinical detector \todo{is it correct to say the image comes from the detector}. Figure \ref{fig:chestPhantomImages} shows the obtained sample pair of transmission map and
processed image.
\todo{proper source of the transmission maps}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/comm_unprocessed_sample.jpg}
        \caption{Real transmission map.}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/comm_processed_sample.jpg}
        \caption{Processed X-ray image (commercial detector).}
    \end{subfigure}
    \caption{Sample pair of images from chest phantom}
    \label{fig:chestPhantomImages}
\end{figure}


\subsection{Diagnostic chest X-ray images}

Throughout the experiments, data was collected from the CheXpert dataset
\cite{chexpert} and NIH Chest X-ray dataset \cite{nih}. The motivation to use these datasets is the diagnostic
labels they include, which can be tied to the simulated transmission maps. However, these labels are not used for
the model. \todo{refer to implementation for transformation details}.

\subsection{Segmentation}

For the implementation of segmentation-based regularizations,
the ChestXDet \cite{chestxdet} model was used to obtain segmentation
labels. Concretely, the segmentation model identifies 13 categories, which are grouped in 3 groups. These groups and their corresponding
labels are listed in Table \ref{tab:segmentationGroups}.

The motivation of the selected groups is the similar absorption coefficients among the different body tissues, which
vary by about 2\% of the dynamic range of an X-ray measurement \todo{maybe the terminology isn't quite right} \cite[p.~54]{epstein2008}.
Thus, if we expect certain ranges of X-ray intensities over different chest regions, these will depend on the absorption coefficients of all tissues transversed.
It is assumed that the projected intensities can vary significantly across regions where: bones are present, lungs are present (air content), and only soft tissues are transversed \todo{I feel I'm not explaining myself}.

\begin{table}[H]
    \centering
    \begin{tabular}{l l}
        \textbf{Group name} & \textbf{Grouped labels} \\
        \hline
        Bone &  Left Clavicle\\
            & Right Clavicle \\
            & Left Scapula\\
            &Right Scapula\\
            &Spine \\
        \hline
        Lung &  Left Lung \\
            & Right Lung \\
            & Left Hilus Pulmonis \\
            & Right Hilus Pulmonis \\
        \hline
        Soft tissue &  Heart \\
        & Aorta \\
        & Mediastinum \\
        & Facies Diaphragmatica \\
        & Weasand
    \end{tabular}
    \caption{Segmentation category groups}
    \label{tab:segmentationGroups}
\end{table}


\subsubsection{Mask groups}

To compute the segmentation masks, the ChestXdet model returns confidence values on a 0 to 1 range. To get hard masks, we include in our
model a threshold parameter. The threshold is used to create binary masks where the values are above it, then the segmentation targets
are joined according to the groups described in Table \ref{tab:segmentationGroups}. The join operation is performed by taking the logical
OR of the masks.

Since the mask groups may contain overlapping regions, a difference is applied to obtain exclusive masks. This is done in an ordered manner,
starting from the groups with higher absorption (bone) up to lower absorption values (lung). This ensures that each pixel is assigned to
the mask that produces the higher attenuation. The complete merging operation is described in algorithm \ref{alg:mergeSegmentationMasks}. Figure
\ref{fig:segmentationMasks} shows a sample set of the processed segmentation masks over a CheXpert image using a threshold of 0.5. \todo{maybe add a figure to explain this algorithm}

\input{algorithms/merge_segmentation_masks.tex}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/segmentation_masks.png}
    \caption{Segmentation masks}
    \label{fig:segmentationMasks}
\end{figure}

\section{Analysis of Transmission Map Properties}

To constrain our solution space, an analysis was done on the limited transmission map data

\begin{description}
    \item[Collimated area range] Only a fraction of the dynamic range of a transmission map
   contains diagnostic information. Figure \ref{fig:transmissionMapsHistograms} shows the histograms
   of the transmission map samples. It can be observed that the captured transmission values are concentrated in
  the range below 0.4, with the outliers corresponding to the regions outside the collimated area. \todo{is 'collimated area' the right term?}
    \begin{figure}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/transmission_maps_histograms.png}
        \caption{Histograms of chest phantom transmission maps}
        \label{fig:transmissionMapsHistograms}
    \end{figure}
    \item[Histogram cluster ranges] Figure \ref{fig:segmentationTransmissionMapsHistograms} shows the histograms
    of different regions. It can be observed that the captured transmission values are concentrated in a closed
    range of intensity values for each group, indicating the presence of distinct tissue types.

    \begin{figure}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/segmentation_txm_histograms.png}
        \caption{Histograms of chest phantom transmission maps grouped by regions}
        \label{fig:segmentationTransmissionMapsHistograms}
    \end{figure}

    \item[Non-uniform histogram] Figure \ref{fig:processedHistogramCompare} shows the histograms of a real pair of transmission map
    and processed image. There is an evident difference with the processed image having more uniform histograms,
    likely resulting from a histogram equalization process. In contrast, transmission maps have higher frequencies
    in the values closer to zero.
    \begin{figure}
        \centering
        \includegraphics[width=1.0\textwidth]{figures/processed_hist_cmp.png}
        \caption{Histograms of processed transmission maps}
        \label{fig:processedHistogramCompare}
    \end{figure}
\end{description}

\section{Forward Operator Model}

Throughout the entire model experimentation, a fixed forward operator is used to represent a
digital radiography processing pipeline. According to the literature, modern iterations on image
processing algorithms maintain a consistent appearance of images that the radiologists are used to
\cite[p.~57]{STA00a}. The motivation then is to work with a model that can produce X-ray images
with an appearance consistent to the existing image datasets. Then, it is expected
that inverting this would produce transmission maps with realistic values.

The goals of image processing can be condensed in the following \cite[p.~149]{Prokop2003}:

\begin{itemize}
    \item to display the full range of attenuation differences in
    the chest,
    \item to optimize spatial resolution of digital chest radio-
    graphs,
    \item to enhance structural contrast in the lungs and medi-
    astinum, and
    \item to suppress image noise.
\end{itemize}

These goals, and choosing transformations that are appropriate for autodifferentiation, rule the design
of our operator $F$ as a composition of several transformations:

\begin{equation}
F = F_n \circ F_{n-1} \circ \cdots \circ F_1
\end{equation}

where the component transformations include:
\begin{description}

\item[Negative logarithm] $F_1(x) = -\log(x + \epsilon)$, where $\epsilon$ is a small constant to avoid numerical instability

    Transmission maps represent the ratio of X-ray intensities $I_1$ and $I_0$,

    \begin{equation}
        \frac{I_1}{I_0} = e^{-\mu x}.
    \end{equation}

    However, processed images operate on the thickness terms $\mu x$
    (explaining the negative relationship between transmission maps and diagnostic images),
    which can be extracted through a negative logarithm.

\item[Windowing] As a way of implementing gradational adjustment, a window function is implemented, that creates
an S-shape lookup table to achieve signal normalization:

\begin{equation}
    F_2(x) = \frac{1}{1+e^{-\gamma \frac{x - c}{w}}},
\end{equation}

where $c$ is the center of the sigmoid function, $w$ is a width parameter, and $\gamma$ is a steepness parameter.
The effect of these parameters and how these translate into a LUT is shown in Figure \ref{fig:windowingParams}.

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/window_params.png}
    \caption{Windowing function with different parameter settings.}
    \label{fig:windowingParams}
\end{figure}

\item[Unsharp masking]
\begin{equation}
    F_4(x) = x + \alpha \cdot (x - G_\sigma * x),
\end{equation}

where $G_\sigma$ is a Gaussian kernel with standard deviation $\sigma$.
This operation achieves edge enhancement, and motivates the search of parameters with non-zero $\alpha$ and $\sigma$ values,
in which case would be equivaling to ignoring this transformation.

\item[Range normalization] $F_3(x) = \frac{x - \min(x)}{\max(x) - \min(x)}$, as a mean to normalize values to the $[0,1]$
range, since operations such as windowing and unsharp masking can lead to values outside this range.

\item[Clipping] $F_5(x) = \min(\max(x, 0), 1)$
\end{description}

The parameter vector $\theta$ includes all parameters of these transformations, such as $\{c, w, \alpha, \sigma\}$.

\section{Optimization Approach}
\subsection{Loss Function Design}
We employ a composite loss function to guide the optimization of transmission maps:
\begin{align}
    \begin{split}
    \ell(Y, F(X, \theta), X, S) = &\ell_{\text{MSE}}(Y, F(X, \theta)) +  \\
                                &\lambda_{\text{TV}} \cdot R_{\text{TV}}(X) + \\
                                &\lambda_{\text{seg}} \cdot R_{\text{seg}}(X, S) + \\
                                &\lambda_{\text{UMS}} \cdot \ell_{\text{UMS}}(Y, F(X, \theta))
    \end{split}
\end{align}

where $\lambda_{\text{TV}}$, $\lambda_{\text{seg}}$, and $\lambda_{\text{UMS}}$ are weighting factors that control the influence of each term, and $S$ represents the segmentation masks.

\subsubsection{Data Fidelity Term}
The mean squared error measures how well our forward model matches the observed processed images:
\begin{align}
    \begin{split}
\ell_{\text{MSE}}(Y, F(X, \theta)) &= \frac{1}{N} \sum_{i=1}^N \|y_i - F(x_i, \theta)\|_2^2
    \end{split}
\end{align}

\subsubsection{Unsharp Mask Similarity Term}
To ensure that the high-frequency details of the reconstructed images match those of the target images, we include an unsharp mask similarity term. Let $G_\sigma$ be a Gaussian kernel with standard deviation $\sigma$. The detail layer extracted by unsharp masking is:
\begin{equation}
D(z) = z - G_\sigma * z
\end{equation}

The unsharp mask similarity loss is then:
\begin{equation}
\ell_{\text{UMS}}(Y, F(X, \theta)) = \frac{1}{N} \sum_{i=1}^N \|D(y_i) - D(F(x_i, \theta))\|_2^2
\end{equation}

This term encourages the preservation of edge and texture details in the reconstructed images, which is particularly important for maintaining diagnostic features.

\subsection{Regularization Terms}

\subsubsection{Total Variation Regularization}
Total variation regularization promotes piecewise smoothness while preserving edges in the reconstructed transmission maps:
\begin{equation}
R_{\text{TV}}(X) = \frac{1}{N} \sum_{i=1}^N R_{\text{TV}}(x_i)
\end{equation}

where for a single image $x_i$:
\begin{equation}
R_{\text{TV}}(x_i) = \frac{1}{2} \left( \|\nabla_h x_i\|_F^2 + \|\nabla_v x_i\|_F^2 \right)
\end{equation}

% Here, $\nabla_h$ and $\nabla_v$ represent horizontal and vertical finite difference operators, and $\|\cdot\|_F$ is the Frobenius norm. In discrete form:
% \begin{equation}
% R_{\text{TV}}(x_i) = \frac{1}{2} \left( \sum_{j=1}^{r-1} \sum_{k=1}^{c} (x_i(j+1,k) - x_i(j,k))^2 + \sum_{j=1}^{r} \sum_{k=1}^{c-1} (x_i(j,k+1) - x_i(j,k))^2 \right)
% \end{equation}

\subsubsection{Segmentation-Based Regularization}
We incorporate physics-based constraints derived from anatomical segmentation to ensure realistic transmission values for different tissue types:

\begin{equation}
R_{\text{seg}}(X, S) = \frac{1}{N} \sum_{i=1}^N R_{\text{seg}}(x_i, S_i)
\end{equation}

where $S_i = \{S_i^1, S_i^2, \ldots, S_i^M\}$ are the segmentation masks for the $M$ anatomical regions in image $i$, and:

\begin{equation}
R_{\text{seg}}(x_i, S_i) = \sum_{m=1}^{M} P_m(x_i, S_i^m)
\end{equation}

The penalty function $P_m$ for region $m$ computes the squared deviation of pixel values from the expected range $[v_{min}^m, v_{max}^m]$:

\begin{equation}
P_m(x_i, S_i^m) = \frac{\sum_{j, k} \left( \max(0, v_{min}^m - x_i(j,k))^2 + \max(0, x_i(j,k) - v_{max}^m)^2 \right) \cdot S_i^m(j,k)}{\sum_{j, k} S_i^m(j,k)}
\end{equation}

This penalizes transmission values that fall outside the expected physical ranges for each anatomical region, normalized by the size of the region.

\subsection{Constraint Enforcement via Projections}
To ensure the optimization process stays within physically plausible regions of the parameter space, we employ projection operators after each gradient update step. These projections enforce hard constraints on both the transmission maps and the forward model parameters.

\subsubsection{Transmission Map Constraints}
Physical principles dictate that transmission values must lie between 0 and 1, where 0 represents complete absorption and 1 represents no absorption. We enforce this using a hypercube projection:

\begin{equation}
P_{\text{hypercube}}(x) = \min(\max(x, 0), 1)
\end{equation}

This ensures that after each optimization step, all pixel values in the transmission maps remain physically valid.

\subsubsection{Forward Model Parameter Constraints}
To ensure the forward model produces realistic processed images, we constrain its parameters within ranges that correspond to clinically relevant image processing operations. These constraints are implemented as:

\begin{equation}
P_{\text{param}}(\theta) = \{P_{\text{box}}(\theta_k; a_k, b_k) \mid k \in \text{keys}(\theta)\}
\end{equation}

where $P_{\text{box}}$ is a box projection that constrains a parameter to lie within a specified interval $[a, b]$:

\begin{equation}
P_{\text{box}}(\theta_k; a_k, b_k) = \min(\max(\theta_k, a_k), b_k)
\end{equation}

Specifically, we apply the following constraints to the forward model parameters:
\begin{itemize}
    \item Windowing parameters: $\text{window\_center} \in [0.1, 0.8]$, $\text{window\_width} \in [0.1, 1.0]$
    \item Sigmoid steepness: $\gamma \in [1, 20]$
    \item Unsharp masking parameters: $\text{low\_sigma} \in [0.5, 10]$, $\text{low\_enhance\_factor} \in [0.3, 1.0]$
\end{itemize}

These constraints ensure that:
\begin{enumerate}
    \item The window center stays within the meaningful range of X-ray transmission values
    \item The window width allows for appropriate contrast adjustment
    \item The sigmoid steepness produces S-curves similar to those used in clinical practice
    \item Unsharp masking uses filter sizes appropriate for enhancing structures at diagnostically relevant scales
    \item Enhancement factors avoid under and over-sharpening artifacts
\end{enumerate}

\subsubsection{Projection in the Optimization Loop}
After each gradient update step, we apply these projections sequentially:

\begin{align}
x_{t+1} &= P_{\text{hypercube}}(x_t - \alpha_x \nabla_x L(x_t, \theta_t)) \\
\theta_{t+1} &= P_{\text{param}}(\theta_t - \alpha_\theta \nabla_\theta L(x_t, \theta_t))
\end{align}

where $\alpha_x$ and $\alpha_\theta$ are learning rates for the transmission maps and forward model parameters, respectively. This projection-based approach ensures that the optimization remains faithful to the physical and practical constraints of the problem, avoiding physically implausible solutions even when the loss function might favor them.

\subsection{Optimization Procedure}
We employ the JAX-based optimization framework with the following steps:
\begin{enumerate}
    \item Initialize transmission maps $X$ and parameters $\theta$ with reasonable values
    \item Compute forward process $F(X, \theta)$
    \item Calculate the composite loss and gradients
    \item Update parameters using Adam optimizer
    \item Apply projection to enforce constraints on $X$ and $\theta$
    \item Repeat until convergence or maximum iterations
\end{enumerate}

The optimization is performed by minimizing the complete loss function with respect to both the transmission maps and the parameters of the forward model.
\section{Evaluation Methodology}
We evaluate our method using several metrics:
\begin{itemize}
    \item \textbf{Image quality metrics}: SSIM, PSNR between reconstructed processed images and originals
    \item \textbf{Forward reconstruction error}: MSE between $F(X, \theta)$ and $Y$
    \item \textbf{Physics compliance}: How well the recovered maps adhere to physical constraints
    \item \textbf{Visual assessment}: Qualitative evaluation by medical imaging experts
\end{itemize}

For the limited cases where ground truth is available (e.g., from phantoms), we also calculate direct error metrics on the recovered transmission maps.
\chapter{Implementation}

\section{Software Architecture}
Our implementation is structured as a Python package with the following components:
\begin{itemize}
    \item \textbf{Data loading and preprocessing}: Modules for handling datasets and preparing images
    \item \textbf{Forward operators}: Differentiable implementations of radiographic processing steps
    \item \textbf{Optimization core}: JAX-based optimization framework with automatic differentiation
    \item \textbf{Regularization functions}: Total variation and other constraints
    \item \textbf{Evaluation metrics}: Functions for assessing reconstruction quality
    \item \textbf{Visualization utilities}: Tools for displaying and analyzing results
\end{itemize}

The code is built using JAX for efficient optimization with automatic differentiation and GPU acceleration.

\section{JAX-Based Optimization Framework}
Our implementation is built around JAX \cite{jax2018github}, which significantly influenced our design decisions and approach to the optimization problem.

\subsection{Automatic Differentiation Requirements}
JAX's automatic differentiation capabilities require all operations in the forward model to be differentiable. This constraint directly impacted our design:

\begin{itemize}
    \item \textbf{Smooth approximations}: We replaced non-differentiable operations with differentiable approximations. For example, we use sigmoid functions instead of hard thresholds.

    \item \textbf{Avoiding discontinuities}: Operations like rounding or thresholding were avoided in favor of continuous alternatives.

    \item \textbf{Epsilon terms}: Small constants were added to prevent numerical instabilities:
    \begin{lstlisting}[language=Python]
    def negative_log(image, eps=1e-6):
        return -jnp.log(jnp.maximum(image, eps))
    \end{lstlisting}
\end{itemize}

% \subsection{Vectorization and Performance Optimization}
% JAX's vectorization capabilities allowed us to efficiently process batches of images:

% \begin{lstlisting}[language=Python]
% # Create a vectorized version of our forward function
% batched_forward = jax.vmap(forward, in_axes=(0, None))

% # Create a vectorized version of our loss function
% def get_batch_mean_loss(unbatched_loss, in_axes=(0, None, 0, 0), **vmap_args):
%     def loss_fn(*args):
%         batched_loss = jax.vmap(unbatched_loss, in_axes=in_axes, **vmap_args)
%         loss_val = batched_loss(*args)
%         return jnp.mean(loss_val)
%     return loss_fn
% \end{lstlisting}

% \subsection{Pure Function Requirements}
% JAX requires pure functions without side effects, influencing our implementation:

% \begin{itemize}
%     \item \textbf{Immutable state}: All state changes are handled through function returns rather than in-place modifications.
%     \item \textbf{Deterministic operations}: Random operations use explicitly managed PRNG keys:
%     \begin{lstlisting}[language=Python]
%     key = jax.random.PRNGKey(hyperparams["PRNGKey"])
%     txm0 = jax.random.uniform(
%         key, minval=minval, maxval=maxval, shape=batch_shape
%     )
%     \end{lstlisting}
% \end{itemize}

% \section{Data Loading and Preprocessing}
% \subsection{CheXpert Dataset Handler}
% We implemented a dataloader for the CheXpert dataset, focusing on frontal chest radiographs:

% \begin{lstlisting}[language=Python, caption=CheXpert dataset loader implementation]
% def load_chexpert(
%     root_dir: str,
%     df_meta_path: str | None = None,
%     img_dir: str | None = None,
%     chexpert_version: str = "240401",
%     limit: int | None = None
% ) -> list[ChexpertMeta]:
%     df_meta_path = (
%         df_meta_path
%         if df_meta_path is not None
%         else os.path.join(root_dir, f"df_chexpert_plus_{chexpert_version}.csv")
%     )
%     img_dir = img_dir if img_dir is not None else os.path.join(root_dir, "PNG")

%     df_meta_path = os.path.abspath(df_meta_path)
%     img_dir = os.path.abspath(img_dir)

%     df_meta = pd.read_csv(df_meta_path)
%     df_meta = df_meta[df_meta["frontal_lateral"] == "Frontal"]

%     images: list[ChexpertMeta] = []

%     for idx, row in df_meta.iterrows():
%         img_path = f"{img_dir}/{row['path_to_image']}"
%         patient_id = str(row["deid_patient_id"])

%         if not os.path.exists(img_path):
%             # chexpert csv reporting jpgs instead of pngs
%             img_path = img_path.replace(".jpg", ".png")
%             if not os.path.exists(img_path):
%                 continue

%         if limit and len(images) >= limit:
%             break

%         img_label = img_path.split("/")[-1]
%         study = img_path[img_path.find("study") :][:6]

%         images.append(
%             {
%                 "image": cv2.imread(img_path, cv2.IMREAD_UNCHANGED) / 255.0,
%                 "image_path": img_path,
%                 "patient_id": patient_id,
%                 "study": study,
%                 "image_label": img_label,
%             }
%         )

%     return images
% \end{lstlisting}

% \subsection{Image Preprocessing}
% Images are preprocessed to ensure consistent dimensions and value ranges:

% \begin{lstlisting}[language=Python, caption=Image preprocessing function]
% def preprocess_chexpert_batch(
%     images, target_size=(512, 512)
% ) -> Float[Array, "batch rows cols"]:
%     """
%     Preprocess a batch of CheXpert images with NumPy.

%     Args:
%         images: Batch of images with shape (batch_size, height, width) or (batch_size, height, width, channels)
%         target_size: Target size (height, width) for all images

%     Returns:
%         Batch of preprocessed images with consistent dimensions
%     """
%     return np.array([center_crop_with_aspect_ratio(img, target_size) for img in images])
% \end{lstlisting}

% \section{Forward Operators Implementation}
% We implemented several operators to model the post-processing pipeline used in digital radiography, ensuring all operations are differentiable for compatibility with JAX:

% \begin{lstlisting}[language=Python, caption=Core forward operators]
% def negative_log(image: Float[Array, "rows cols"], eps=1e-6):
%     return -jnp.log(jnp.maximum(image, eps))

% def windowing(image, window_center, window_width, gamma):
%     x = image
%     x = (x - window_center) / window_width
%     x = jax.nn.sigmoid(x) ** gamma
%     return x

% def unsharp_masking(image, sigma, enhance_factor):
%     x = jnp.expand_dims(image, axis=2)
%     kernel_size = 2 * sigma
%     blurred = gaussian_blur(x, sigma, kernel_size, padding="SAME")
%     x = (x - enhance_factor * blurred) / (1.0 - enhance_factor)
%     return x.squeeze()

% def clipping(image):
%     return jnp.clip(image, 0.0, 1.0)

% def range_normalize(image):
%     x = image
%     return (x - x.min()) / (x.max() - x.min())
% \end{lstlisting}

% \section{Optimization Core}
% The core optimization function implements gradient descent with support for various optimizers and regularization terms. The optimization process leverages JAX's automatic differentiation to efficiently compute gradients:

% \begin{lstlisting}[language=Python, caption=Base optimization function]
% def base_optimize(
%     target: BatchT,
%     txm0: BatchT,
%     w0: WeightsT,
%     loss_fn: LossFnT,
%     forward_fn: ForwardFnT,
%     project_fn: ProjectFnT | None = None,
%     optimizer_builder=optax.adam,
%     constant_weights=False,
%     lr=0.001,
%     n_steps=500,
%     loss_logger=None,
%     eps=1e-8,
% ) -> OptimizationRetT:
%     def loss_call(weights, tx_maps, target):
%         pred = forward_fn(tx_maps, weights)
%         loss = loss_fn(tx_maps, weights, pred, target)

%         if loss_logger:
%             loss_logger(loss.item(), tx_maps, weights, pred, target)

%         assert pred.shape == target.shape, (
%             f"Shapes do not match: {pred.shape} != {target.shape}"
%         )

%         return loss

%     optimizer = optimizer_builder(learning_rate=lr)
%     state = (txm0, w0)
%     opt_state = optimizer.init(state)
%     losses = []
%     prev_state = (None, None)

%     def update(state, opt_state, target):
%         original_state = (state, opt_state)
%         tx_maps, weights = state

%         loss_value_and_grad = jax.value_and_grad(loss_call, argnums=(0, 1))
%         loss, (weight_grads, tx_grads) = loss_value_and_grad(weights, tx_maps, target)
%         grads = (tx_grads, weight_grads)

%         updates, new_opt_state = optimizer.update(grads, opt_state)
%         updates_txm, updates_weights = updates

%         txm_new_state = optax.apply_updates(tx_maps, updates_txm)
%         weights_new_state = weights
%         if not constant_weights:
%             weights_new_state = optax.apply_updates(weights, updates_weights)

%         new_state = (txm_new_state, weights_new_state)
%         if project_fn:
%             new_state = project_fn(txm_new_state, weights_new_state)

%         if jnp.isnan(loss):
%             new_state, new_opt_state = original_state

%         return loss, new_state, new_opt_state

%     for step in range(n_steps):
%         if step > 2 and jnp.abs(losses[-1] - losses[-2]) < eps:
%             wandb.log({"convergence_steps": step})
%             print(f"Converged after {step} steps")
%             break

%         loss, state, opt_state = update(state, opt_state, target)
%         losses.append(loss)

%         if jnp.isnan(loss):
%             state, loss = prev_state
%             print("Loss is NaN. Last loss:", loss)
%             break

%         if step % 100 == 0:
%             print(f"\nStep {step}, Loss: {loss:.6f}")

%         prev_state = (state, loss)

%     if state is None:
%         return None, losses

%     return state, losses
% \end{lstlisting}

% \section{Constraint Enforcement}
% We use Optax's projection operators to enforce constraints while maintaining differentiability:

% \begin{lstlisting}[language=Python, caption=Projection function for constraint enforcement]
% def projection(txm_state, weights_state):
%     new_txm_state = optax.projections.projection_hypercube(txm_state)
%     new_weights_state = optax.projections.projection_non_negative(weights_state)
%     new_weights_state["low_sigma"] = optax.projections.projection_box(
%         weights_state["low_sigma"], 0.1, 10
%     )
%     new_weights_state["low_enhance_factor"] = optax.projections.projection_box(
%         new_weights_state["low_enhance_factor"], 0.1, 1.0
%     )
%     return new_txm_state, new_weights_state
% \end{lstlisting}

% \section{Loss Functions and Metrics}
% We implemented several loss functions and metrics for evaluation:

% \begin{lstlisting}[language=Python, caption=Loss functions and metrics]
% def mse(pred, target):
%     return jnp.mean((pred - target) ** 2)

% def total_variation(image):
%     reg = (jnp.diff(image, axis=0) ** 2).mean() + (jnp.diff(image, axis=1) ** 2).mean()
%     return 0.5 * reg

% def ssim(x, y, L=1.0):
%     C1 = (0.01 * L) ** 2
%     C2 = (0.03 * L) ** 2

%     mu_x = jnp.mean(x)
%     mu_y = jnp.mean(y)

%     sigma_x = jnp.mean((x - mu_x) ** 2)
%     sigma_y = jnp.mean((y - mu_y) ** 2)
%     sigma_xy = jnp.mean((x - mu_x) * (y - mu_y))

%     num = (2 * mu_x * mu_y + C1) * (2 * sigma_xy + C2)
%     den = (mu_x**2 + mu_y**2 + C1) * (sigma_x + sigma_y + C2)

%     return jnp.clip(num / den, 0.0, 1.0)
% \end{lstlisting}

% \section{Hyperparameter Search Configuration}
% We use Weights \& Biases for hyperparameter search:

% \begin{lstlisting}[language=Python, caption=Hyperparameter sweep configuration]
% sweep_config = {
%     "name": "unknown-transform-sweep-v2",
%     "method": "bayes",
%     "metric": {"name": "mse", "goal": "minimize"},
%     "parameters": {
%         "lr": {"min": 1e-5, "max": 1e-1},
%         "n_steps": {"values": [500, 700, 1200]},
%         "total_variation": {"values": [0.0, 0.1, 0.01, 0.001]},
%         "PRNGKey": {"values": [0, 42]},
%         "tm_distribution": {"values": ["uniform", "normal"]},
%         "tm_init_range": {
%             "values": [
%                 (1e-6, 0.1),
%                 (1e-6, 0.5),
%                 (1e-6, 1.0),
%                 (0.5, 1.0),
%             ]
%         },
%         # Fixed/metadata parameters
%         "loss": {"value": "MSE + TV"},
%         "initialization": {"value": "uniform random"},
%         "window_center_init_range": {"value": [0.1, 0.9]},
%         "window_width_init_range": {"value": [0.1, 0.9]},
%         "gamma_init_range": {"value": [0.1, 3.0]},
%         "eps": {"value": 1e-6},
%     },
% }
% \end{lstlisting}

\section{Experimental Setup}
Our experiments are configured to explore different models and hyperparameters:

\begin{itemize}
    \item \textbf{Known vs. Unknown Transformation}: We compare scenarios where the forward transformation is known versus when it must be inferred
    \item \textbf{Single vs. Batch Processing}: We evaluate optimizing for individual images versus batches
    \item \textbf{Regularization Strength}: We vary the total variation regularization parameter
    \item \textbf{Initialization Strategies}: We test different initialization methods for the transmission maps
\end{itemize}

Each experiment is run with multiple random seeds to ensure robustness, and results are logged to Weights \& Biases for analysis.
\chapter{Results and Analysis}
\label{ch:resultsAndAnalysis}
\sweExpl{svensk: Resultat och Analys}

\engExpl{Sometimes this is split into two chapters.\\Keep in mind: How you are going to evaluate what you have done? What are your metrics?\\Analysis of your data and proposed solution\\Does this meet the goals which you had when you started?}

In this chapter, we present the results and discuss them.

\sweExpl{I detta kapitel presenterar vi resultaten och diskutera dem.\\Ibland delas detta upp i två kapitel.\\Hur du ska utvärdera vad du har gjort? Vad är din statistik?\\Analys av data och föreslagen lösning\\Innebär detta att uppfyllelse av de mål som du hade när du började?}

\section{Major results}
\sweExpl{Huvudsakliga resultat}

Some statistics of the delay measurements are shown in Table~\ref{tab:delayMeasurements}.
The delay has been computed from the time the GET request is received until the response is sent.

\sweExpl{Lite statistik av fördröjningsmätningarna visas i Tabell~\ref{tab:delayMeasurements}. Förseningen har beräknats från den tidpunkt då begäran GET tas emot fram till svaret skickas.}

\begin{table}[!ht]
  \begin{center}
    \caption{Delay measurement statistics}
    \label{tab:delayMeasurements}
    \begin{tabular}{l|S[table-format=4.2]|S[table-format=3.2]} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
      \textbf{Configuration} & \textbf{Average delay (ns)} & \textbf{Median delay (ns)}\\
      \hline
      1 & 467.35 & 450.10\\
      2 & 1687.5 & 901.23\\
    \end{tabular}
  \end{center}
\end{table}

Table \ref{tab:ping_results} shows the measurement of round trip times from four hosts to and from a server.
\begin{table}[ht!]
\caption[RTT for 4 hosts]{Result for the ping measurements of RTT for 4 hosts}
\label{tab:ping_results}
\vspace{1em}
\centering
\begin{tabular}{l *{4}{S[table-format=2.3]}}
{} & \multicolumn{4}{c}{host to server RTT in ms} \\
\cmidrule{2-5}
Host & \multicolumn{1}{c}{min}  & \multicolumn{1}{c}{avg} & \multicolumn{1}{c}{max} & \multicolumn{1}{c}{mdev} \\
\midrule
h1 & 5.625 & 5.625 & 5.625 & 0.0 \\
h2 & 2.909 & 2.909 & 1.909 & 0.0 \\
h3 & 5.007 & 5.007 & 5.007 & 0.0 \\
h4 & 2.308 & 2.308 & 2.308 & 0.0 \\
\midrule
\end{tabular}
\end{table}
\FloatBarrier

\sweExpl{Fördröj mätstatistik}
\sweExpl{Konfiguration | Genomsnittlig fördröjning (ns) | Median fördröjning (ns)}

Figure \ref{fig:processing_vs_payload_length} shows an example of the performance as measured in the experiments.

\begin{figure}[!ht]
% GNUPLOT: LaTeX picture
\setlength{\unitlength}{0.240900pt}
\ifx\plotpoint\undefined\newsavebox{\plotpoint}\fi
\begin{picture}(1500,900)(0,0)
\sbox{\plotpoint}{\rule[-0.200pt]{0.400pt}{0.400pt}}%
\put(171.0,131.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,131){\makebox(0,0)[r]{ 1.5}}
\put(1419.0,131.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,212.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,212){\makebox(0,0)[r]{ 2}}
\put(1419.0,212.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,292.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,292){\makebox(0,0)[r]{ 2.5}}
\put(1419.0,292.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,373.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,373){\makebox(0,0)[r]{ 3}}
\put(1419.0,373.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,454.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,454){\makebox(0,0)[r]{ 3.5}}
\put(1419.0,454.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,534.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,534){\makebox(0,0)[r]{ 4}}
\put(1419.0,534.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,615.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,615){\makebox(0,0)[r]{ 4.5}}
\put(1419.0,615.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,695.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,695){\makebox(0,0)[r]{ 5}}
\put(1419.0,695.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,776.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(151,776){\makebox(0,0)[r]{ 5.5}}
\put(1419.0,776.0){\rule[-0.200pt]{4.818pt}{0.400pt}}
\put(171.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(171,90){\makebox(0,0){ 0}}
\put(171.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(298.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(298,90){\makebox(0,0){ 10}}
\put(298.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(425.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(425,90){\makebox(0,0){ 20}}
\put(425.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(551.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(551,90){\makebox(0,0){ 30}}
\put(551.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(678.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(678,90){\makebox(0,0){ 40}}
\put(678.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(805.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(805,90){\makebox(0,0){ 50}}
\put(805.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(932.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(932,90){\makebox(0,0){ 60}}
\put(932.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1059.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1059,90){\makebox(0,0){ 70}}
\put(1059.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1185.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1185,90){\makebox(0,0){ 80}}
\put(1185.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1312.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1312,90){\makebox(0,0){ 90}}
\put(1312.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1439.0,131.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(1439,90){\makebox(0,0){ 100}}
\put(1439.0,756.0){\rule[-0.200pt]{0.400pt}{4.818pt}}
\put(171.0,131.0){\rule[-0.200pt]{0.400pt}{155.380pt}}
\put(171.0,131.0){\rule[-0.200pt]{305.461pt}{0.400pt}}
\put(1439.0,131.0){\rule[-0.200pt]{0.400pt}{155.380pt}}
\put(171.0,776.0){\rule[-0.200pt]{305.461pt}{0.400pt}}
\put(30,453){\rotatebox{-270}{\makebox(0,0){Processing time (ms)}}}
\put(805,29){\makebox(0,0){Payload size (bytes)}}
\put(868.0,131.0){\rule[-0.200pt]{0.400pt}{84.074pt}}
\put(995.0,131.0){\rule[-0.200pt]{0.400pt}{98.287pt}}
\put(1173.0,131.0){\rule[-0.200pt]{0.400pt}{118.041pt}}
\put(1325.0,131.0){\rule[-0.200pt]{0.400pt}{134.904pt}}
\put(1350.0,131.0){\rule[-0.200pt]{0.400pt}{137.795pt}}
\put(1439.0,131.0){\rule[-0.200pt]{0.400pt}{155.380pt}}
\end{picture}
\caption[A GNUplot figure]{Processing time vs. payload length}\vspace{0.5cm}
\label{fig:processing_vs_payload_length}
\end{figure}
\FloatBarrier

Given these measurements, we can calculate our processing bit rate as the inverse of the time it takes to process an additional byte divided by 8 bits per byte:

\[
	\text{bit rate} = \frac{1}{\frac{\text{time}_{\text{byte}}}{8}} = 20.03 \quad kb/s
\]

\Cref{tab:majorMarkupLMDetailedResult} shows another table in which some values have been set in bold (using \textbackslash B) to emphasize them. Note how the \texttt{S} formatting has been modified so that it considers the weight of the characters and this is able to decimal align even these hold-faced numbers with the numbers in the column above them.

\begin{table}[!ht]
    \centering
    \caption{Median values of sandwich attributes}
    \label{tab:majorMarkupLMDetailedResult}
    \begin{tabular}{l *{2}{S[detect-weight,mode=text,table-format=3.2]}}
        & \multicolumn{2}{c}{\textbf{sites}}\\
        \cmidrule{2-3}
        \textbf{Attribute} & \textbf{A} & \textbf{B} \\
        \midrule
        price (in SEK) & 36.5 & 71.3 \\
        protean (g) & 97.2 & 100.0 \\
        salt (mg) & 9.7 & 9.3 \\
        \hline
        \textbf{Average customer rating in \%} & \B 82.2 & \B 89.9 \\
        \midrule
    \end{tabular}
\end{table}
\FloatBarrier


\Needspace*{4\baselineskip}
\Cref{fig:stackedrust} shows a stacked bar chart using pgfplots. It illustrates how easy it is to take a set of data and make a stacked bar plot. One of the features is the shifted values -- this is very useful when the bar itself is too small to put the value into.

\pgfplotstableread{
Label Numbers  Refs  Struct/Enum  Heap  Arrays
cratesio 70.04 19.83 8.31 1.3 0.52
librs 49.26 30.49 10.80 7.92 1.53
rustc 55.01 24.80 11.54 6.16 2.49
}\testdata


\pgfkeys{
    /pgf/number format/.cd,
    fixed,
    fixed zerofill,
    precision=2
}
\begin{figure}[ht!]
    \centering
    \scalebox{0.9}{
    \begin{tikzpicture}
    \begin{axis}[
        ybar stacked,
        %reverse legend,
        reverse legend=false,
        %https://tex.stackexchange.com/questions/88892/pgfplots-bar-plot-spacing-inbetween-bars
        enlarge x limits=0.4,
	    bar width=45pt,
        /pgfplots/nodes near coords*/.append style={
        every node near coord/.style={
            color=black,
            font=\small,
            name=X,
%            shift={
%                (50pt,25pt)
%                },
            xshift={50pt},
                yshift ={
                ifthenelse((\plotnum == 4), 30pt,20pt)},
            },
            scatter/@post marker code/.append code={
                \node(Y){};
                \draw(X)--(Y.center);
            }
        },
	    nodes near coords,
        bar shift=5pt,
        ymin=0,
        ymax=115,
        xtick=data,
        width=1\textwidth,
        legend style={draw=none},
        legend image post style={scale=2.0},
        legend style={
            at={(0.5,-0.2)},
            anchor=north,
            legend columns=-2,
            font=\large,
            %mark size=20pt,
        },
        ylabel=Percentage points (\%),
        xticklabels from table={\testdata}{Label},
        xticklabel style={rotate=30},
    ]
    \addplot  table [y=Numbers, meta=Label, x expr=\coordindex] {\testdata};
    \addlegendentry{Numbers}
    \addplot table [y=Refs, meta=Label, x expr=\coordindex] {\testdata};
    \addlegendentry{Refs}
    \addplot  table [y=Struct/Enum, meta=Label, x expr=\coordindex] {\testdata};
    \addlegendentry{Struct/Enum}
    \addplot  table [y=Heap, meta=Label, x expr=\coordindex] {\testdata};
    \addlegendentry{Heap}
    \addplot  table [y=Arrays, meta=Label, x expr=\coordindex] {\testdata};
    \addlegendentry{Arrays}
    \end{axis}
    \end{tikzpicture}}
\caption{Rust types distribution for the compiler, crates.io, and lib.rs.
(percentage) - appears here with the permission of the author - see the thesis at \url{https://urn.kb.se/resolve?urn=urn\%3Anbn\%3Ase\%3Akth\%3Adiva-332124}}
\label{fig:stackedrust}
\end{figure}
\FloatBarrier



\section{Reliability Analysis}
\sweExpl{Analys av tillförlitlighet\\
Tillförlitlighet i metod och data}

\section{Validity Analysis}
\sweExpl{Analys av validitet\\
Validitet i metod och data}

\cleardoublepage
\chapter{Discussion}
\label{ch:discussion}
\sweExpl{Diskussion\\
Förbättringsförslag?}
\generalExpl{This can be a separate chapter or a section in the previous chapter.}

\cleardoublepage
\chapter{Conclusions and Future work}
\label{ch:conclusionsAndFutureWork}
\sweExpl{Slutsats och framtida arbete}

\generalExpl{Add text to introduce the subsections of this chapter.}

\section{Conclusions}
\label{sec:conclusions}
\sweExpl{Slutsatser}
\engExpl{Describe the conclusions (reflect on the whole introduction given in Chapter 1).}



\engExpl{Discuss the positive effects and the drawbacks.\\
Describe the evaluation of the results of the degree project.\\
Did you meet your goals?\\
What insights have you gained?\\
What suggestions can you give to others working in this area?\\
If you had it to do again, what would you have done differently?}

\sweExpl{Uppfyllde du dina mål?\\
Vilka insikter har du fått?\\
Vilka förslag kan du ge till andra som arbetar inom detta område?
Om du skulle göra detta igen, vad skulle du ha gjort annorlunda?}

\section{Limitations}
\label{sec:limitations}
\sweExpl{Begränsande faktorer\\Vad gjorde du som begränsade dina ansträngningar? Vilka är begränsningarna i dina resultat?}
\engExpl{What did you find that limited your efforts? What are the limitations of your results?}


\section{Future work}
\label{sec:futureWork}
\sweExpl{Vad du har kvar ogjort?\\Vad är nästa självklara saker som ska göras?\\Vad tips kan du ge till nästa person som kommer att följa upp på ditt arbete?}
\engExpl{Describe valid future work that you or someone else could or should do.\\
Consider: What you have left undone? What are the next obvious things to be done? What hints can you give to the next person who is going to follow up on your work?}



Due to the breadth of the problem, only some of the initial goals have been
met. In these section we will focus on some of the remaining issues that
should be addressed in future work. ...

\subsection{What has been left undone?}
\label{what-has-been-left-undone}

The prototype does not address the third requirment, \ie a yearly unavailability of less than 3 minutes; this remains an open problem. ...

\subsubsection{Cost analysis}
\generalExpl{Example of a missing component}
The current prototype works, but the performance from a cost perspective makes this an impractical solution. Future work must reduce the cost of this solution; to do so, a cost analysis needs to first be done. ...

\subsubsection{Security}
\generalExpl{Example of a missing component}
A future research effort is needed to address the security holes that results from using a self-signed certificate. Page filling text mass. Page filling text mass. ...


\subsection{Next obvious things to be done}

In particular, the author of this thesis wishes to point out xxxxxx remains as a problem to be solved. Solving this problem is the next thing that should be done. ...

\section{Reflections}
\label{sec:reflections}
\sweExpl{Reflektioner}
\sweExpl{Vilka är de relevanta ekonomiska, sociala, miljömässiga och etiska aspekter av ditt arbete?}
\engExpl{What are the relevant economic, social,
  environmental, and ethical aspects of your work?
}



One of the most important results is the reduction in the amount of
energy required to process each packet while at the same time reducing the
time required to process each packet.

The thesis contributes to the \gls{UN}\enspace\glspl{SDG} numbers 1 and 9 by
xxxx.




\noindent\rule{\textwidth}{0.4mm}
\engExpl{In the references, let Zotero or other tool fill this in for you. I suggest an extended version of the IEEE style, to include URLs, DOIs, ISBNs, etc., to make it easier for your reader to find them. This will make life easier for your opponents and examiner. \\IEEE Editorial Style Manual: \url{https://www.ieee.org/content/dam/ieee-org/ieee/web/org/conferences/style_references_manual.pdf}}
\sweExpl{Låt Zotero eller annat verktyg fylla i det här för dig. Jag föreslår en utökad version av IEEE stil - att inkludera webbadresser, DOI, ISBN osv. - för att göra det lättare för läsaren att hitta dem. Detta kommer att göra livet lättare för dina opponenter och examinator.}

\cleardoublepage
% Print the bibliography (and make it appear in the table of contents)
\renewcommand{\bibname}{References}


\ifbiblatex
    %\typeout{Biblatex current language is \currentlang}
    \printbibliography[heading=bibintoc]
\else
    \phantomsection  % make it include a hyperref - see https://tex.stackexchange.com/a/98995
    \addcontentsline{toc}{chapter}{References}
    \bibliography{references}
\fi



\warningExpl{If you do not have an appendix, do not include the \textbackslash cleardoublepage command below; otherwise, the last page number in the metadata will be one too large.}
\cleardoublepage
\appendix
\renewcommand{\chaptermark}[1]{\markboth{Appendix \thechapter\relax:\thinspace\relax#1}{}}
\chapter{Supporting materials}
\label{sec:supportingMaterial}
\generalExpl{Here is a place to add supporting material that can help others build upon your work. You can include files as attachments to the PDF file or indirectly via URLs. Alternatively, consider adding supporting material uploaded as separate files in DiVA.}

% Attach the BibTeX for your references to make it easy for a reader to find and use them
The BibTeX references used in this thesis are attached. \attachfile[description={references.bib}]{references.bib}

% Attach source code file(s) or add a URL to the github or other repository
Some source code relevant to this project can be found at \url{https://github.com/gqmaguirejr/E-learning} and \url{https://github.com/gqmaguirejr/Canvas-tools}.

Your reader can access the attached (embedded) files using a PDF tool such as Adobe Acrobat Reader using the paperclip icon in the left menu, as shown in \Cref{fig:PDFreaderPaperclipExample} or by right-clicking on the push-pin icon in the PDF file and then using the menu to save the embedded file as shown in \Cref{fig:PDFreaderPushpinExample}.

An argument for including supporting material in the PDF file is that it will be available to anyone who has a copy of the PDF file. As a result, they do not have to look elsewhere for this material. This comes at the cost of a larger PDF file. However, the embedded files are encoded into a compressed stream within the PDF file; thus, reducing the number of additional bytes. For example, the references.bib file that was used in this example is \SI{10617}{\byte} in size but only occupies \SI{4261}{\byte} in the PDF file.

\warningExpl{DiVA is limited to $\approx$\SI{1}{\giga\byte} for each supporting file. If you have very large amounts of supporting material, you will probably want to use one of the data repositories. For additional help with this, contact KTH Library via
\href{mailto:researchdata@kth.se}{researchdata@kth.se}.\\As of Spring 2024, there are plans to migrate this supporting data from DiVA to a research data repository.
}

\begin{figure}[!ht]
  \begin{center}
    \includegraphics[width=0.50\textwidth]{README_notes/pdf-viewer-attached-files.png}
  \end{center}
  \caption{Adobe Acrobat Reader using the paperclip icon for the attached references.bib file}
  \label{fig:PDFreaderPaperclipExample}
\end{figure}
\FloatBarrier

\begin{figure}[!ht]
  \begin{center}
    \includegraphics[width=0.99\textwidth]{README_notes/Bib-save-embedded-example.png}
  \end{center}
  \caption{Adobe Acrobat Reader after right-clicking on the push-pin icon for the attached references.bib file}
  \label{fig:PDFreaderPushpinExample}
\end{figure}
\FloatBarrier
\cleardoublepage

\chapter{Something Extra}
\sweExpl{svensk: Extra Material som Bilaga}

\section{Just for testing KTH colors}
\ifdigitaloutput
    \textbf{You have selected to optimize for digital output}
\else
    \textbf{You have selected to optimize for print output}
\fi
\begin{itemize}[noitemsep]
    \item Primary color
    \begin{itemize}
    \item \textcolor{kth-blue}{kth-blue \ifdigitaloutput
    actually Deep sea
    \fi} {\color{kth-blue} \rule{0.3\linewidth}{1mm} }\\

    \item \textcolor{kth-blue80}{kth-blue80} {\color{kth-blue80} \rule{0.3\linewidth}{1mm} }\\
\end{itemize}

\item  Secondary colors
\begin{itemize}[noitemsep]
    \item \textcolor{kth-lightblue}{kth-lightblue \ifdigitaloutput
    actually Stratosphere
    \fi} {\color{kth-lightblue} \rule{0.3\linewidth}{1mm} }\\

    \item \textcolor{kth-lightred}{kth-lightred \ifdigitaloutput
    actually Fluorescence\fi} {\color{kth-lightred} \rule{0.3\linewidth}{1mm} }\\

    \item \textcolor{kth-lightred80}{kth-lightred80} {\color{kth-lightred80} \rule{0.3\linewidth}{1mm} }\\

    \item \textcolor{kth-lightgreen}{kth-lightgreen \ifdigitaloutput
    actually Front-lawn\fi} {\color{kth-lightgreen} \rule{0.3\linewidth}{1mm} }\\

    \item \textcolor{kth-coolgray}{kth-coolgray \ifdigitaloutput
    actually Office\fi} {\color{kth-coolgray} \rule{0.3\linewidth}{1mm} }\\

    \item \textcolor{kth-coolgray80}{kth-coolgray80} {\color{kth-coolgray80} \rule{0.3\linewidth}{1mm} }
\end{itemize}
\end{itemize}

\textcolor{black}{black} {\color{black} \rule{\linewidth}{1mm} }

% Include an example of using nomenclature
\ifnomenclature
    \cleardoublepage
    \chapter{Main equations}
    \label{ch:NomenclatureExamples}
    This appendix gives some examples of equations that are used throughout this thesis.
    \section{A simple example}
    The following example is adapted from Figure 1 of the documentation for the package nomencl (\url{https://ctan.org/pkg/nomencl}).
    \begin{equation}\label{eq:mainEq}
    a=\frac{N}{A}
    \end{equation}
    \nomenclature{$a$}{The number of angels per unit area\nomrefeq}%       %% include the equation number in the list
    \nomenclature{$N$}{The number of angels per needle point\nomrefpage}%  %% include the page number in the list
    \nomenclature{$A$}{The area of the needle point}%
    The equation $\sigma = m a$%
    \nomenclature{$\sigma$}{The total mass of angels per unit area\nomrefeqpage}%
    \nomenclature{$m$}{The mass of one angel}
follows easily from \Cref{eq:mainEq}.

    \section{An even simpler example}
    The formula for the diameter of a circle is shown in \Cref{eq:secondEq} area of a circle in \cref{eq:thirdEq}.
    \begin{equation}\label{eq:secondEq}
    D_{circle}=2\pi r
    \end{equation}
    \nomenclature{$D_{circle}$}{The diameter of a circle\nomrefeqpage}%
    \nomenclature{$r$}{The radius of a circle\nomrefeqpage}%

    \begin{equation}\label{eq:thirdEq}
    A_{circle}=\pi r^2
    \end{equation}
    \nomenclature{$A_{circle}$}{The area of a circle\nomrefeqpage}%

    Some more text that refers to \eqref{eq:thirdEq}.
\fi  %% end of nomenclature example

\cleardoublepage
% Information for authors
%\include{README_author}
\subfile{README_author}

\cleardoublepage
% information about the template for everyone
\input{README_notes/README_notes}

\begin{comment}
% information for examiners
\ifxeorlua
\cleardoublepage
\input{README_notes/README_examiner_notes}
\fi
\end{comment}

\begin{comment}
% Information for administrators
\ifxeorlua
\cleardoublepage
\input{README_notes/README_for_administrators.tex}
\fi
\end{comment}

\begin{comment}
% Information for Course Coordinators
\ifxeorlua
\cleardoublepage
\input{README_notes/README_for_course_coordinators}
\fi
\end{comment}

%% The following label is necessary for computing the last page number of the body of the report to include in the "For DIVA" information
\label{pg:lastPageofMainmatter}

\cleardoublepage
\clearpage\thispagestyle{empty}\mbox{} % empty page with backcover on the other side
\kthbackcover
\fancyhead{}  % Do not use header on this extra page or pages
\section*{€€€€ For DIVA €€€€}
\lstset{numbers=none} %% remove any list line numbering
\divainfo{pg:lastPageofPreface}{pg:lastPageofMainmatter}

% If there is an acronyms.tex file,
% add it to the end of the For DIVA information
% so that it can be used with the abstracts
% Note that the option "nolol" stops it from being listed in the List of Listings

% The following bit of ugliness is because of the problems PDFLaTeX has handling a non-breaking hyphen
% unless it is converted to UTF-8 encoding.
% If you do not use such characters in your acronyms, this could be simplified.
\ifxeorlua
\IfFileExists{lib/acronyms.tex}{
\section*{acronyms.tex}
\lstinputlisting[language={[LaTeX]TeX}, nolol, basicstyle=\ttfamily\color{black},
commentstyle=\color{black}, backgroundcolor=\color{white}]{lib/acronyms.tex}
}
{}
\else
\IfFileExists{lib/acronyms-for-pdflatex.tex}{
\section*{acronyms.tex}
\lstinputlisting[language={[LaTeX]TeX}, nolol, basicstyle=\ttfamily\color{black},
commentstyle=\color{black}, backgroundcolor=\color{white}]{lib/acronyms-for-pdflatex.tex}
}
{}
\fi


\end{document}
