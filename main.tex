
%%
%% forked from https://gits-15.sys.kth.se/giampi/kthlatex kthlatex-0.2rc4 on 2020-02-13
%% expanded upon by Gerald Q. Maguire Jr.
%% This template has been adapted by Anders Sjögren to the University
%% Engineering Program in Computer Science at KTH ICT. This adaptation was
%% Many thanks to others who have provided constructive input regarding the template.

% Make it possible to conditionally depend on the TeX engine used
\RequirePackage{ifxetex}
\RequirePackage{ifluatex}
\newif\ifxeorlua
\ifxetex\xeorluatrue\fi
\ifluatex\xeorluatrue\fi

\ifxeorlua
% The following is to ensure that the PDF uses a recent version rather than the typical PDF 1-5
%  This same version of PDF should be set as an option for hyperef

\RequirePackage{expl3}
\ExplSyntaxOn
%pdf_version_gset:n{2.0}
%\pdf_version_gset:n{1.5}

%% Alternatively, if you have a LaTeX newer than June 2022, you can use the following. However, then you have to remove the pdfversion from hyperef. It also breaks hyperxmp. So perhaps it is too early to try using it!
%\DocumentMetadata
%{
%% testphase = phase-I, % tagging without paragraph tagging
% testphase = phase-II % tagging with paragraph tagging and other new stuff.
%pdfversion = 2.0 % pdfversion must be set here.
%}

% Optionally, you can set the uncompress flag to make it easier to examine the PDF
%\pdf_uncompress: % to check the pdf
\ExplSyntaxOff
\else
\RequirePackage{expl3}
\ExplSyntaxOn
%\pdf_version_gset:n{2.0}
\pdf_version_gset:n{1.5}
\ExplSyntaxOff
\fi

%% Define a pair of commands to disable and reenable specific packages - see https://tex.stackexchange.com/questions/39415/unload-a-latex-package
\makeatletter
\newcommand{\disablepackage}[2]{%
  \disable@package@load{#1}{#2}%
}
\newcommand{\reenablepackage}[1]{%
  \reenable@package@load{#1}%
}
\makeatother
%% To avoid the warning: "Package transparent Warning: Loading aborted, because pdfTeX is not running in PDF mode."
\ifxeorlua
\disablepackage{transparent}{}
\fi



%% The template is designed to handle a thesis in English or Swedish
% set the default language to english or swedish by passing an option to the documentclass - this handles the inside title page
% To optimize for digital output (this changes the color palette add the option: digitaloutput
% To use \ifnomenclature add the option nomenclature
% To use bibtex or biblatex - include one of these as an option
\documentclass[nomenclature, english, bibtex]{kththesis}
%\documentclass[swedish, biblatex]{kththesis}
% if pdflatex \usepackage[utf8]{inputenc}


%% Conventions for todo notes:
% Informational
%% \generalExpl{Comments/directions/... in English}
\newcommand*{\generalExpl}[1]{\todo[inline]{#1}}

% Language-specific information (currently in English or Swedish)
\newcommand*{\engExpl}[1]{\todo[inline, backgroundcolor=kth-lightgreen40]{#1}} %% \engExpl{English descriptions about formatting}
\newcommand*{\sweExpl}[1]{\todo[inline, backgroundcolor=kth-lightblue40]{#1}}  %% % \sweExpl{Text på svenska}

% warnings
\newcommand*{\warningExpl}[1]{\todo[inline, backgroundcolor=kth-lightred40]{#1}} %% \warningExpl{warnings}

% Uncomment to hide specific comments, to hide **all** ToDos add `final` to
% document class
% \renewcommand\warningExpl[1]{}
% \renewcommand\generalExpl[1]{}
% \renewcommand\engExpl[1]{}
% For example uncommenting the following line hides the Swedish language explanations
% \renewcommand\sweExpl[1]{}


% \usepackage[style=numeric,sorting=none,backend=biber]{biblatex}
\ifbiblatex
    %\usepackage[language=english,bibstyle=authoryear,citestyle=authoryear, maxbibnames=99]{biblatex}
    % Alternatively you might use another style, such as IEEE and use citestyle=numeric-comp  to put multiple citations in a single pair of square brackets
    \usepackage[style=ieee,citestyle=numeric-comp]{biblatex}
    \addbibresource{references}
    %\DeclareLanguageMapping{norsk}{norwegian}
\else
    % The line(s) below are for BibTeX
    \bibliographystyle{bibstyle/myIEEEtran}
    %\bibliographystyle{apalike}
\fi


% include a variety of packages that are useful
\input{lib/includes}
\input{lib/kthcolors}

%\glsdisablehyper
%\makeglossaries
%\makenoidxglossaries
%\input{lib/acronyms}                %load the acronyms file

\input{lib/defines}  % load some additional definitions to make writing more consistent

% The following is needed in conjunction with generating the DiVA data with abstracts and keywords using the scontents package and a modified listings environment
%\usepackage{listings}   %  already included
\ExplSyntaxOn
\newcommand\typestoredx[2]{\expandafter\__scontents_typestored_internal:nn\expandafter{#1} {#2}}
\ExplSyntaxOff
\makeatletter
\let\verbatimsc\@undefined
\let\endverbatimsc\@undefined
\lst@AddToHook{Init}{\hyphenpenalty=50\relax}
\makeatother


\lstnewenvironment{verbatimsc}
    {
    \lstset{%
        basicstyle=\ttfamily\tiny,
        backgroundcolor=\color{white},
        %basicstyle=\tiny,
        %columns=fullflexible,
        columns=[l]fixed,
        language=[LaTeX]TeX,
        %numbers=left,
        %numberstyle=\tiny\color{gray},
        keywordstyle=\color{red},
        breaklines=true,                 % sets automatic line breaking
        breakatwhitespace=true,          % sets if automatic breaks should only happen at whitespace
        %keepspaces=false,
        breakindent=0em,
        %fancyvrb=true,
        frame=none,                     % turn off any box
        postbreak={}                    % turn off any hook arrow for continuation lines
    }
}{}

%% Add some more keywords to bring out the structure more
\lstdefinestyle{[LaTeX]TeX}{
morekeywords={begin, todo, textbf, textit, texttt}
}

%% definition of new command for bytefield package
\newcommand{\colorbitbox}[3]{%
	\rlap{\bitbox{#2}{\color{#1}\rule{\width}{\height}}}%
	\bitbox{#2}{#3}}




% define a left aligned table cell that is ragged right
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}

% Because backref is not compatible with biblatex
\ifbiblatex
    \usepackage[plainpages=false]{hyperref}
\else
    \usepackage[
    backref=page,
    pagebackref=false,
    plainpages=false,
                            % PDF related options
    unicode=true,           % Unicode encoded PDF strings
    bookmarks=true,         % generate bookmarks in PDF files
    bookmarksopen=false,    % Do not automatically open the bookmarks in the PDF reading program
    pdfpagemode=UseNone,    % None, UseOutlines, UseThumbs, or FullScreen
    destlabel,              % better naming of destinations
    pdfencoding=auto,       % for unicode in
    ]{hyperref}
    \makeatletter
    \ltx@ifpackageloaded{attachfile2}{
    % cannot use backref if one is using attachfile
    }
    {\usepackage{backref}
    %
    % Customize list of backreferences.
    % From https://tex.stackexchange.com/a/183735/1340
    \renewcommand*{\backref}[1]{}
    \renewcommand*{\backrefalt}[4]{%
    \ifcase #1%
          \or [Page~#2.]%
          \else [Pages~#2.]%
    \fi%
    }
    }
    \makeatother

\fi
\usepackage[all]{hypcap}	%% prevents an issue related to hyperref and caption linking

%% Acronyms
% note that nonumberlist - removes the cross references to the pages where the acronym appears
% note that super will set the descriptions text aligned
% note that nomain - does not produce a main glossary, thus only acronyms will be in the glossary
% note that nopostdot - will prevent there being a period at the end of each entry
\usepackage[acronym, style=super, section=section, nonumberlist, nomain,
nopostdot]{glossaries}
\setlength{\glsdescwidth}{0.75\textwidth}
\usepackage[]{glossaries-extra}
\ifinswedish
    %\usepackage{glossaries-swedish}
\fi

%% For use with the README_notes
% Define a new type of glossary so that the acronyms defined in the README_notes document can be distinct from those in the thesis template
% the tlg, tld, and dn will be the file extensions used for this glossary
\newglossary[tlg]{readme}{tld}{tdn}{README acronyms}


\input{lib/includes-after-hyperref}

%\glsdisablehyper
\makeglossaries
%\makenoidxglossaries

% The following bit of ugliness is because of the problems PDFLaTeX has handling a non-breaking hyphen
% unless it is converted to UTF-8 encoding.
% If you do not use such characters in your acronyms, this could be simplified to just include the acronyms file.
\ifxeorlua
\input{lib/acronyms}                %load the acronyms file
\else
\input{lib/acronyms-for-pdflatex}
\fi


% insert the configuration information with author(s), examiner, supervisor(s), ...
\input{custom_configuration}

\title{Chest X-ray Transmission Map Reconstruction}
\subtitle{Constrained Optimization to Invert a Family of Image Processing Algorithms}

% give the alternative title - i.e., if the thesis is in English, then give a Swedish title
\alttitle{Detta är den svenska översättningen av titeln}
\altsubtitle{Detta är den svenska översättningen av undertiteln}
% alternative, if the thesis is in Swedish, then give an English title
%\alttitle{This is the English translation of the title}
%\altsubtitle{This is the English translation of the subtitle}

% Enter the English and Swedish keywords here for use in the PDF metadata _and_ for later use
% following the respective abstract.
% Try to put the words in the same order in both languages to facilitate matching. For example:
\EnglishKeywords{Nonlinear Optimization, Medical Imaging, Digital Image Processing}
\SwedishKeywords{Canvas Lärplattform, Dockerbehållare, Prestandajustering}

%%%%% For the oral presentation
%% Add this information once your examiner has scheduled your oral presentation
\presentationDateAndTimeISO{2022-03-15 13:00}
\presentationLanguage{eng}
\presentationRoom{via Zoom https://kth-se.zoom.us/j/ddddddddddd}
\presentationAddress{Isafjordsgatan 22 (Kistagången 16)}
\presentationCity{Stockholm}

% When there are multiple opponents, separate their names with '\&'
% Opponent's information
\opponentsNames{A. B. Normal \& A. X. E. Normalè}

% Once a thesis is approved by the examiner, add the TRITA number
% The TRITA number for a thesis consists of two parts: a series (unique to each school)
% and the number in the series, which is formatted as the year followed by a colon and
% then a unique series number for the thesis - starting with 1 each year.
\trita{TRITA -- EECS-EX}{2024:0000}

% Put the title, author, and keyword information into the PDF meta information
\input{lib/pdf_related_includes}


% the custom colors and the commands are defined in defines.tex
\hypersetup{
	colorlinks  = true,
	breaklinks  = true,
	linkcolor   = \linkscolor,
	urlcolor    = \urlscolor,
	citecolor   = \refscolor,
	anchorcolor = black
}

\ifnomenclature
% The following lines make the page numbers and equations hyperlinks in the Nomenclature list
\renewcommand*{\pagedeclaration}[1]{\unskip, \dotfill\hyperlink{page.#1}{page\nobreakspace#1}}
% The following does not work correctly, as the name of the cross-reference is incorrect
%\renewcommand*{\eqdeclaration}[1]{, see equation\nobreakspace(\hyperlink{equation.#1}{#1})}

% You can also change the page heading for the nomenclature
\renewcommand{\nomname}{List of Symbols Used}

% You can even add customization text before the list
\renewcommand{\nompreamble}{The following symbols will be later used within the body of the thesis.}
\makenomenclature
\fi

%
% The commands below are to configure JSON listings
%
% format for JSON listings
\colorlet{punct}{red!60!black}
\definecolor{delim}{RGB}{20,105,176}
\definecolor{numb}{RGB}{106, 109, 32}
\definecolor{string}{RGB}{0, 0, 0}

\lstdefinelanguage{json}{
    numbers=none,
    numberstyle=\small,
    frame=none,
    rulecolor=\color{black},
    showspaces=false,
    showtabs=false,
    breaklines=true,
    postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{gray}\hookrightarrow\space}},
    breakatwhitespace=true,
    basicstyle=\ttfamily\small,
    extendedchars=false,
    upquote=true,
    morestring=[b]",
    stringstyle=\color{string},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1}
      {’}{{\char13}}1,
}

\lstdefinelanguage{XML}
{
  basicstyle=\ttfamily\color{blue}\bfseries\small,
  morestring=[b]",
  morestring=[s]{>}{<},
  morecomment=[s]{<?}{?>},
  stringstyle=\color{black},
  identifierstyle=\color{blue},
  keywordstyle=\color{cyan},
  breaklines=true,
  postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{gray}\hookrightarrow\space}},
  breakatwhitespace=true,
  morekeywords={xmlns,version,type}% list your attributes here
}

% If you use both listing and lstlisting environments - the following makes them both use the same counter and the same formatting in the List of Listings
\makeatletter
\AtBeginDocument{\let\c@listing\c@lstlisting}
\AtBeginDocument{\let\l@listing\l@lstlisting}
\makeatother

% Change the heading for the "List of Listings" to simply "Listings"
\renewcommand{\lstlistlistingname}{Listings}

% number each listing within the chapter
\numberwithin{listing}{chapter}

\usepackage{subfiles}

% To have Creative Commons (CC) license and logos use the doclicense package
% Note that the lowercase version of the license has to be used in the modifier
% i.e., one of by, by-nc, by-nd, by-nc-nd, by-sa, by-nc-sa, zero.
% For background see:
% https://www.kb.se/samverkan-och-utveckling/oppen-tillgang-och-bibsamkonsortiet/open-access-and-bibsam-consortium/open-access/creative-commons-faq-for-researchers.html
% https://kib.ki.se/en/publish-analyse/publish-your-article-open-access/open-licence-your-publication-cc
\begin{comment}
\usepackage[
    type={CC},
    %modifier={by-nc-nd},
    %version={4.0},
    modifier={by-nc},
    imagemodifier={-eu-88x31},  % to get Euro symbol rather than Dollar sign
    hyphenation={RaggedRight},
    version={4.0},
    %modifier={zero},
    %version={1.0},
]{doclicense}
\end{comment}

\begin{document}
%\selectlanguage{swedish}
%
\selectlanguage{english}

%%% Set the numbering for the title page to a numbering series not in the preface or body
\pagenumbering{alph}
\kthcover
\clearpage\thispagestyle{empty}\mbox{} % empty back of front cover
\titlepage

% If you do not want to have a bookinfo page, comment out the line saying \bookinfopage and add a \cleardoublepage
% If you want a bookinfo page: you will get a copyright notice, unless you have used the doclicense package in which case you will get a Creative Commons license. To include the doclicense package, uncomment the configuration of this package above and configure it with your choice of license.
\bookinfopage

% Frontmatter includes the abstracts and table-of-contents
\frontmatter
\setcounter{page}{1}
\begin{abstract}
% The first abstract should be in the language of the thesis.
% Abstract fungerar på svenska också.
  \markboth{\abstractname}{}
\begin{scontents}[store-env=lang]
eng
\end{scontents}
%%% The contents of the abstract (between the begin and end of scontents) will be saved in LaTeX format
%%% and output on the page(s) at the end of the thesis with information for DiVA facilitating the correct
%%% entry of the meta data for your thesis.
%%% These page(s) will be removed before the thesis is inserted into DiVA.
% ----- EXPLANATION -----
% \engExpl{All theses at KTH are \textbf{required} to have an abstract in both \textit{English} and \textit{Swedish}.}

% \engExpl{Exchange students may want to include one or more abstracts in the language(s) used in their home institutions to avoid the need to write another thesis when returning to their home institution.}

% \generalExpl{Keep in mind that most of your potential readers are only going to read your \texttt{title} and \texttt{abstract}. This is why the abstract must give them enough information so that they can decide if this document is relevant to them or not. Otherwise, the likely default choice is to ignore the rest of your document.\\
% An abstract should stand on its own, \ie no citations, cross-references to the body of the document, acronyms must be spelled out, \ldots .\\Write this early and revise as necessary. This will help keep you focused on what you are trying to do.}

\begin{scontents}[store-env=abstracts,print-env=true]
% \generalExpl{Enter your abstract here!}
An abstract is (typically) about 250 and 350 words (1/2 A4-page) with the following components:
% key parts of the abstract
\begin{itemize}
  \item What is the topic area? (optional) Introduces the subject area for the project.
  \item Short problem statement
  \item Why was this problem worth a Bachelor's/Master’s thesis project? (\ie, why is the problem both significant and of a suitable degree of difficulty for a Bachelor's/Master’s thesis project? Why has no one else solved it yet?)
  \item How did you solve the problem? What was your method/insight?
  \item Results/Conclusions/Consequences/Impact: What are your key results/\linebreak[4]conclusions? What will others do based on your results? What can be done now that you have finished - that could not be done before your thesis project was completed?
\end{itemize}

\end{scontents}
% \engExpl{The following are some notes about what can be included (in terms of LaTeX) in your abstract.}
Choice of typeface with \textbackslash textit, \textbackslash textbf, and \textbackslash texttt:  \textit{x}, \textbf{x}, and \texttt{x}.

Text superscripts and subscripts with \textbackslash textsubscript and \textbackslash textsuperscript: A\textsubscript{x} and A\textsuperscript{x}.

Some symbols that you might find useful are available, such as: \textbackslash textregistered, \textbackslash texttrademark, and \textbackslash textcopyright. For example,
the copyright symbol: \textbackslash textcopyright Maguire 2022 results in \textcopyright Maguire 2022. Additionally, here are some examples of text superscripts (which can be combined with some symbols): \textbackslash textsuperscript\{99m\}Tc, A\textbackslash textsuperscript\{*\}, A\textbackslash textsuperscript\{\textbackslash textregistered\}, and A\textbackslash texttrademark resulting in \textsuperscript{99m}Tc, A\textsuperscript{*}, A\textsuperscript{\textregistered}, and A\texttrademark. Two examples of subscripts are: H\textbackslash textsubscript\{2\}O and CO\textbackslash textsubscript\{2\} which produce  H\textsubscript{2}O and CO\textsubscript{2}.

You can use simple environments with begin and end: itemize and enumerate and within these use instances of \textbackslash item.

The following commands can be used: \textbackslash eg, \textbackslash Eg, \textbackslash ie, \textbackslash Ie, \textbackslash etc, and \textbackslash etal: \eg, \Eg, \ie, \Ie, \etc, and \etal.

The following commands for numbering with lowercase Roman numerals: \textbackslash first, \textbackslash Second, \textbackslash third, \textbackslash fourth, \textbackslash fifth, \textbackslash sixth, \textbackslash seventh, and \textbackslash eighth: \first, \Second, \third, \fourth, \fifth, \sixth, \seventh, and \eighth. Note that the second case is set with a capital 'S' to avoid conflicts with the use of second of as a unit in the \texttt{siunitx} package.

Equations using \textbackslash( xxxx \textbackslash) or \textbackslash[ xxxx \textbackslash] can be used in the abstract. For example: \( (C_5O_2H_8)_n \)
or \[ \int_{a}^{b} x^2 \,dx \]
Note that you \textbf{cannot} use an equation between dollar signs.


Even LaTeX comments can be handled by using a backslash to quote the percent symbol, for example: \% comment.
Note that one can include percentages, such as: 51\% or \SI{51}{\percent}.

\subsection*{Keywords}
\begin{scontents}[store-env=keywords,print-env=true]
% If you set the EnglishKeywords earlier, you can retrieve them with:
\InsertKeywords{english}
% If you did not set the EnglishKeywords earlier then simply enter the keywords here:
% comma separate keywords, such as: Canvas Learning Management System, Docker containers, Performance tuning
\end{scontents}
\engExpl{\textbf{Choosing good keywords can help others to locate your paper, thesis, dissertation, \ldots and related work.}}
Choose the most specific keyword from those used in your domain, see for example: the ACM Computing Classification System ({\small \url{https://www.acm.org/publications/computing-classification-system/how-to-use})},
the IEEE Taxonomy ({\small \url{https://www.ieee.org/publications/services/thesaurus-thank-you.html}}), PhySH (Physics Subject Headings)\linebreak[4] ({\small \url{https://physh.aps.org/}}), \ldots or keyword selection tools such as the  National Library of Medicine's Medical Subject Headings (MeSH)  ({\small \url{https://www.nlm.nih.gov/mesh/authors.html}}) or Google's Keyword Tool ({\small \url{https://keywordtool.io/}})\\

\textbf{Formatting the keywords}:
\begin{itemize}
  \item The first letter of a keyword should be set with a capital letter and proper names should be capitalized as usual.
  \item Spell out acronyms and abbreviations.
  \item Avoid "stop words" - as they generally carry little or no information.
  \item List your keywords separated by commas (``,'').
\end{itemize}
Since you should have both English and Swedish keywords - you might think of ordering the keywords in corresponding order (\ie, so that the n\textsuperscript{th} word in each list correspond) - this makes it easier to mechanically find matching keywords.
\end{abstract}
\cleardoublepage
\babelpolyLangStart{swedish}
\begin{abstract}
    \markboth{\abstractname}{}
\begin{scontents}[store-env=lang]
swe
\end{scontents}
% \warningExpl{Inside the following scontents environment, you cannot use a \textbackslash include{filename} as the command rather than the file contents will end up in the for DiVA information. Additionally, you should not use a straight double quote character in the abstracts or keywords, use two single quote characters instead.}
\begin{scontents}[store-env=abstracts,print-env=true]
\generalExpl{Enter your Swedish abstract or summary here!}
% \sweExpl{Alla avhandlingar vid KTH \textbf{måste ha} ett abstrakt på både \textit{engelska} och \textit{svenska}.\\
% Om du skriver din avhandling på svenska ska detta göras först (och placera det som det första abstraktet) - och du bör revidera det vid behov.}

% ----- EXPLANATION -----
% \engExpl{If you are writing your thesis in English, you can leave this until the draft version that goes to your opponent for the written opposition. In this way, you can provide the English and Swedish abstract/summary information that can be used in the announcement for your oral presentation.\\If you are writing your thesis in English, then this section can be a summary targeted at a more general reader. However, if you are writing your thesis in Swedish, then the reverse is true – your abstract should be for your target audience, while an English summary can be written targeted at a more general audience.\\This means that the English abstract and Swedish sammnfattning
% or Swedish abstract and English summary need not be literal translations of each other.}

% \warningExpl{Do not use the \textbackslash glspl\{\} command in an abstract that is not in English, as my programs do not know how to generate plurals in other languages. Instead, you will need to spell these terms out or give the proper plural form. In fact, it is a good idea not to use the glossary commands at all in an abstract/summary in a language other than the language used in the \texttt{acronyms.tex file} - since the glossary package does \textbf{not} support use of more than one language.}

% \engExpl{The abstract in the language used for the thesis should be the first abstract, while the Summary/Sammanfattning in the other language can follow}
\end{scontents}
\subsection*{Nyckelord}
\begin{scontents}[store-env=keywords,print-env=true]
% SwedishKeywords were set earlier, hence we can use alternative 2
\InsertKeywords{swedish}
\end{scontents}
\sweExpl{Nyckelord som beskriver innehållet i uppsatsen eller rapporten}
\end{abstract}
\babelpolyLangStop{swedish}

\cleardoublepage
% ----- EXPLANATION -----
% \generalExpl{If you are an exchange student, use the relevant language or languages for abstracts for your home university, as this will often avoid the need for writing another thesis for your home university.\\
% If you are fluent in other languages, feel free to add the abstracts in one or more of them.}
% \engExpl{Note that you may need to augment the set of languages used in \texttt{polyglossia} or
% \texttt{babel} (see the file \texttt{kththesis.cls}). The following languages include those languages that were used in theses at KTH in 2018-2019, except for one in Chinese.\\
% Remove those versions of abstracts that you do not need.\\
% If you add a new language, when specifying the language for the abstract, use the three-letter ISO 639-2 Code – specifically the "B" (bibliographic) variant of these codes (note that this is the same language code used in DiVA).}

% \babelpolyLangStart{spanish}
% \begin{abstract}
%     \markboth{\abstractname}{}
% \begin{scontents}[store-env=lang]
% spa
% \end{scontents}
% \begin{scontents}[store-env=abstracts,print-env=true]
% Résumé en espagnol.
% \end{scontents}
% \subsection*{Palabras claves}
% \begin{scontents}[store-env=keywords,print-env=true]
% 5-6 Palabras claves
% \end{scontents}
% \end{abstract}
% \babelpolyLangStop{spanish}
% \cleardoublepage

\section*{Acknowledgments}
\markboth{Acknowledgments}{}

% ----- EXPLANATION -----
% \sweExpl{Författarnas tack}

% \engExpl{It is nice to acknowledge the people that have helped you. It is
%   also necessary to acknowledge any special permissions that you have gotten –
%   for example, getting permission from the copyright owner to reproduce a
%   figure. In this case, you should acknowledge them and this permission here
%   and in the figure’s caption. \\
%   Note: If you do \textbf{not} have the copyright owner’s permission, then you \textbf{cannot} use any copyrighted figures/tables/\ldots . Unless stated otherwise all figures/tables/\ldots are generally copyrighted.
% }
% \sweExpl{I detta kapitel kan du ev nämna något om
%   din bakgrund om det påverkar rapporten på något sätt. Har du t ex inte
%   möjlighet att skriva perfekt svenska för att du är nyanländ till landet kan
%   det vara på sin plats att nämna detta här. OBS, detta får dock inte vara en
%   ursäkt för att lämna in en rapport med undermåligt språk, undermålig grammatik och
%   stavning (t ex får fel som en automatisk stavningskontroll och
%   grammatikkontroll kan upptäcka inte förekomma)\\
% En dualism som måste hanteras i hela rapporten och projektet
% }

I would like to thank xxxx for having yyyy. Or in the case of two authors:\\
We would like to thank xxxx for having yyyy.

\acknowlegmentssignature

\fancypagestyle{plain}{}
\renewcommand{\chaptermark}[1]{ \markboth{#1}{}}
\tableofcontents
  \markboth{\contentsname}{}

\cleardoublepage
\listoffigures

\cleardoublepage

\listoftables
\cleardoublepage
\lstlistoflistings\engExpl{If you have listings in your thesis. If not, then remove this preface page.}
\cleardoublepage
% Align the text expansion of the glossary entries
\newglossarystyle{mylong}{%
  \setglossarystyle{long}%
  \renewenvironment{theglossary}%
     {\begin{longtable}[l]{@{}p{\dimexpr 2cm-\tabcolsep}p{0.8\hsize}}}% <-- change the value here
     {\end{longtable}}%
 }
%\glsaddall
%\printglossaries[type=\acronymtype, title={List of acronyms}]
\printglossary[style=mylong, type=\acronymtype, title={List of acronyms and abbreviations}]
%\printglossary[type=\acronymtype, title={List of acronyms and abbreviations}]

%\printnoidxglossary[style=mylong, title={List of acronyms and abbreviations}]
\engExpl{The list of acronyms and abbreviations should be in alphabetical order based on the spelling of the acronym or abbreviation.
}

% if the nomenclature option was specified, then include the nomenclature page(s)
\ifnomenclature
    \cleardoublepage
    % Output the nomenclature list
    \printnomenclature
\fi

%% The following label is essential to know the page number of the last page of the preface
%% It is used to compute the data for the "For DIVA" pages
\label{pg:lastPageofPreface}
% Mainmatter is where the actual contents of the thesis goes
\mainmatter
\glsresetall
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\selectlanguage{english}
\chapter{Introduction}

\section{Background}


Digital radiography systems and their technological advancements for better X-ray image acquisition play a vital role in
enabling accurate disease diagnosis and progression monitoring. Naturally, a demand for automated image interpretation
and diagnosis has emerged, followed by a need of large and diverse X-ray image datasets. Collecting
such data is however a challenge given the complexity of the acquisition, annotation process,
and the tendency of these datasets to have an imbalance when underrepresenting certain conditions, such as normal cases,
while simultaneously overrepresenting pneumonia \cite{ansariMitigatingRiskMedical2025}.

Another source of bias is the different capture conditions across (or even within) datasets. These can be found
in images that go through different enhancing methods, device quality, and patient positioning. Some of these
differences are depicted in \autoref{fig:imagingBias}. And even though these differences can be mitigated by
mixing datasets, this method can also lead to models discriminating on the source dataset \cite{arias-garzonBiasesAssociatedDatabase2023}.
Most importantly, any conditions that are not represented in the data can lead to existing models performing poorly.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/imaging_bias.jpeg}
    \caption{Examples of different capture conditions across datasets, taken from \cite[Figure~6]{arias-garzonBiasesAssociatedDatabase2023}.}
    \label{fig:imagingBias}
\end{figure}

A potential solution to these dataset biases lies in the fundamental physics of X-ray imaging.
In projection radiography, all imaging systems—whether using film, computed radiography phosphor plates, or flat panel
detectors—measure the spatial distribution of X-ray radiation incident on the detector after passing through
the patient \cite{Seibert3}. Different detector technologies employ various methods to convert the incoming x-rays
into a visual representation, potentially losing information in the process. Furthermore, digital
detectors perform image processing algorithms to increase the image quality for diagnosis, but also contribute to the
differences in image appearance across different detectors.

This observation suggests a promising approach: by recovering these fundamental transmission patterns from
processed clinical images, existing annotated datasets could be translated into detector-agnostic representations.
This could enable Machine Learning and Deep Learning models to generalize across different imaging systems without
requiring new data for each specific detector. This thesis explores the feasibility and effectiveness of transmission
map recovery as a preprocessing step for improving the generalization of chest X-ray analysis models.

\section{Problem Statement}
\subsection{Original Problem Definition}

All projection radiography technologies, and their subsequent processing pipelines operate on the same
\textit{transmission maps}, which represent the X-ray radiation that passed through the patient's body.
For digital radiography systems, if no processing were applied and the images captured were directly transformed
into gray levels, they would appear as extremely dark due to the lack of contrast \cite[p.~148]{Prokop2003}.
Thus, clinical grade equipment manufacturers (e.g., Siemens, Philips, GE) apply proprietary digital image processing
algorithms that include non-linear operations—likely including a combination of denoising, frequency filtering,
and dynamic range adjustments. These operations form a "black-box" that varies across manufacturers, and even on an
image-by-image basis.

This describes the primary goal of this project: to develop a framework that inverts the image processing transformations
from high-quality, chest x-ray images. This will recover the latent images described in \autoref{fig:image_processing_pipeline}
that will enable the extension of exiting labeled datasets via the siulation of non-represented radiography sysyems.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/latent_image.jpg}
    \caption{X-ray image acquisition pipeline. All projection radiography capture the same x-ray energies and
   form a latent image, which is distorted by the systems' methods to capture x-rays and subsequent processing steps.
    Taken from \cite[p.~13]{Seibert3}}
    \label{fig:image_processing_pipeline}
\end{figure}

\subsection{Scientific and Engineering Challenges}

The problem to solve lies in the category of inverse problems, and has several critical challenges:

\begin{description}
    \item[Ill-posedness]
        A single processed image can map back to multiple possible "realistic" transmission maps.
        The same holds on the other way around, where a single transmission map can map back to multiple processed images.
       This makes the inverse problem fundamentally ill-posed.
    \item[Diversity of processing approaches]
        Each manufacturer implements proprietary image processing pipelines that differ in their specific algorithms and parameters.
    \item[Limited ground truth] We lack labeled data showing the original transmission maps corresponding to processed images.
    \item[Scaling requirements] We need to process many images to build a representative dataset.
\end{description}

\section{Purpose}

The primary use case that motivates this project is the development of alternative X-ray detector technologies.
Large chest X-ray datasets like CheXpert \cite{chexpert} and ChestX-ray8 \cite{nih} collect images from large
and modern hospitals with extensive archives of X-ray imaging studies. While these can drive X-ray models to be
used in most clinical environments, they may not be effective on resource-limited settings. These include portable,
or cost-effective alternatives.

Building large datasets for these subset of technologies is unlikely, given this has been possible due to
modern archiving systems (PACS) keeping long histories of X-ray studies \cite[p.~3462]{nih}. Being able
to translate existing datasets into representations of any other imaging system will bring existing and
coming advancements in X-ray related automation to all scenarios.

\todo{add references to x-ray detector accessibility}

\section{Goals}

Concretely, the expected outcomes of this project are the following:

\begin{enumerate}
    \item Analyze limited X-ray transmission map data to identify and characterize the essential features that define radiologically realistic representations.
    \item Characterize the diversity of image processing pipelines applied to these maps in commercial systems.
    \item Develop and evaluate optimization algorithms that can recover plausible transmission maps from processed images.
\end{enumerate}

\section{Thesis Structure}

The remainder of this thesis is organized as follows: Chapter 2 presents background information on X-ray transmission and
digital image processing that are relevant to the problem and motivate the proposed method. Chapter 3
presents the mathematical description of the problem and the . Chapter 4 includes implementation
details and computational settings. Chapter 5 presents experimental results, and Chapter 6 discusses implications
and directions for future work.

\chapter{X-ray Transmission}
\label{sec:xrayTransmissionModel}

The lack of ground truth and prior knowledge for our problem incentivizes the analysis of the underlying
physics of X-ray images, as well as further research on the types of operations that are done in the black boxes
we aim to invert. This chapter presents the basic principles of X-ray transmission, the process of X-ray image
acquisition, and the main goals and common algorithms that are used to process radiographic images. The
knowledge presented in this section motivates the design choices made to develop the proposed method.

\section{X-ray intensity and Beer's law}

X-rays are thought of as a flux of very high-energy, electromagnetic radiation. The x-ray beam is
described by a vector valued function $I(x)$. The direction of $I$ at $x$ is the direction of the flux at $x$ and
its magnitude,
\begin{equation}
    I (x) = \lVert I(x) \rVert
\end{equation}
is the intensity of the beam. If $dS$ is an infinitesimal surface element at $x$ of area
$|dS|$, placed at right angles to $I(x)$, then the energy-per-unit-time passing through $dS$ is \cite[p.~56]{epstein2008}.

\begin{equation}
    I (x) |dS|.
\end{equation}

When x-rays encounter any form of matter, they are partly transmitted and partly absorbed.
The fractional decrease in the intensity $I$ of an x-ray beam as it passes
through any homogeneous substance is proportional to the distance traversed $x$
and the material encountered\cite[p.~11]{cullityElementsXrayDiffraction2014}. The intensity
$I$ of the x-ray beam satisfies Beer's law:

\begin{equation}
    \frac{dI}{ds} = -\mu(x)I,
    \label{eq:BeerLambert}
\end{equation}

where $s$ is the arc-length along the straight-line trajectory of the x-ray beam.
Each material encountered has a characteristic \textit{linear attenuation coefficient} $\mu$ for x-rays of a
given energy, and is dependent on the substance composition, density, and the wavelength of the x-rays \cite[p.~57]{epstein2008}.

For a homogeneous medium of thickness $x$ with linear attenuation coefficient $\mu$, if radiation of intensity
$I_{\text{in}}$ is incident upon the medium, the transmitted intensity $I_{\text{out}}$ is given by:

\begin{equation}
I_{\text{out}} = I_{\text{in}} \cdot e^{-\mu(x)}
\label{eq:beer_lambert}
\end{equation}

\section{X-ray Image acquisition}

In the context of X-ray imaging, we measure intensities at the detector plane. Let

\begin{itemize}
\item $I_0(x,y)$ denote the intensity measured at detector position $(x,y)$ in the absence
    of any object (the reference or flat-field measurement), and
\item $I(x,y)$ denote the intensity measured at detector position $(x,y)$ with the object present.
\end{itemize}

Then, for radiation traversing a heterogeneous medium, such as body tissues, along a ray path $L$
from source to detector position $(x,y)$, the relation of the incident and transmitted intensity is
given by:

\begin{equation}
I(x,y) = I_0(x,y) \cdot \exp\left(-\int_L \mu(s) \, ds\right)
\label{eq:beer_lambert_imaging}
\end{equation}

where $\mu(s)$ represents the spatially varying attenuation coefficient along the ray path \cite[p.~57]{epstein2008}.

In a real measurement, the x-ray source is turned on for a known period of time. The total energy
$I$ incident on the object along a given line $l$ is known. The total energy, $I_0$, emerging from the object
along $l$ is then measured by an x-ray detector. Integrating Beer’s law we obtain \cite[p.~60]{epstein2008}

\begin{equation}
    -\log \frac{I(x,y)}{I_0(x,y)} = \int_l \mu(s) \, ds,
\end{equation}

where $\frac{I(x, y)}{I_0(x, y)}$ is the \textit{transmission map}, which describes the fraction of the
incident radiation that transmits through the patient at each position $(x, y)$, and applying
the negative logarithm captures the energy absorbed by the object.

For a medium composed of $n$ distinct homogeneous regions with attenuation coefficients
$\{\mu_i\}_{i=1}^n$ and thicknesses $\{d_i\}_{i=1}^n$ along a given ray path,
a simplified model can be expressed as
\begin{equation}
    I(x,y) &= I_0(x,y) \cdot \exp\left(-\sum_{i=1}^n \mu_i d_i\right).
    \label{eq:discrete_materials}
\end{equation}

Note that ideal x-ray image acquisition and the described model assume a point x-ray source, a straight line
trajectory from the source through the object, and complete detection of the x-ray beam that strikes the detector.
These are not realistic assumptions, however, clinical detectors apply methods to counteract nonideal conditions
\cite[p.~9]{Seibert3}.

\section{Application to Human Chest Imaging}
\label{sec:human_chest_imaging}

In the context of chest radiography, the human thorax can be modeled as a composition of distinct, but
known tissue types, and therefore, different anatomical structures have different attenuation coefficients.
Bone has a much higher attenuation coefficient than soft tissue, and different soft tissues have slightly
different coefficients. More precisely, \autoref{fig:tissue_attenuation} shows the attenuation coefficients
of common body tissue types. The table is presented in a dimensionless quantity called a
Hounsfield unit, which is a measure relative to the attenuation coefficient of water, defined as
\cite[p.~54]{epstein2008}

\begin{equation}
    H_{\text{tissue}} = 1000 \cdot \frac{\mu_{\text{tissue}} − \mu_{\text{water}}}{\mu_{\text{water}}}
\end{equation}

\begin{figure}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        Tissue Type & Attenuation Coefficient \\
        \hline
        water & 0 \\
        \hline
        air & -1000 \\
        \hline
        bone & 1086 \\
        \hline
        blood & 53 \\
        \hline
        fat & -61 \\
        \hline
        breast tissue & 9 \\
        \hline
        muscle & 41 \\
        \hline
        soft tissue & 51 \\
        \hline
    \end{tabular}
    \caption{Body tissue attenuation coefficients in Hounsfeld units. Adapted from \cite[p.~54]{epstein2008}}
    \label{fig:tissue_attenuation}
\end{figure}


\section{Digital Image Processing in Radiography}
\label{sec:DigitalImageProcessing}


\acrshort{DR} systems can capture the wide attenuation differences between lungs and mediastinum due to
their wide dynamic range and linear response to th incident radiation. However, it leads to a lack of contrast on
the direct conversion to gray values. At the same time, image sharpness may not be as good as in screen-film
due to the pixel size constraint \cite[p.~148]{Prokop2003}. \acrshort{FSR} systems, despite having greater spatial
resolution, they have a non-linear (S-shaped) response, leading to under- or overexposed images\cite[p.~551]{vuylstekeMultiscaleImageContrast1994}.
Regardless of the imaging modality, digital image processing is not only used to take full advantage of the positive
characteristics of the radiography systems, but also to amend the intrinsic limitations of each detector.

To overcome this, different algorithms are applied to the captured x-ray image, with the goals of:

\begin{itemize}
    \item displaying the full range of attenuation differences in the chest,
    \item optimizing spatial resolution of digital chest radiographs,
    \item enhancing structural contrast in the lungs and mediastinum, and
    \item suppressing image noise \cite[p.~149]{Prokop2003}.
\end{itemize}


\subsection{Dynamic range reduction}

Particularly for \acrshort{DR} systems, the dynamic range is large, only a small portion of it contains diagnostically
relevant information. The image histogram can be 'stretched' via a \acrlong{LUT} (\acrshort{LUT}) that exclude
any values outside the relevant range. As depicted in \autoref{fig:histogram_stretching}, this improves the overall
image contrast, and normalizes for different doses and patient body types \cite[p.~151]{Prokop2003}.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/LUT.png}
    \caption{Gradational curves applied to normalize signal over different x-ray doses.\cite[Figure~3]{Prokop2003}}
    \label{fig:histogram_stretching}
\end{figure}


\subsection{Spatial Frequency Processing}

As a mean to improve structural contrast and achieve edge enhancement, spatial frequency algorithms are applied.
Unlike dynamic range operations, these types of processings can vary drastically across manufacturers, and
are driven by multiple parameters that can also change on a per-image basis. Nevertheles, despite the algorithms
changing by vendor (Agfa [Mortsel, Belgium] uses MUSICA; Fuji [Tokyo, Japan] uses Gradation; and Kodak uses Tonescaling),
they all share the same effect and purpose \cite[p.~119]{carterDigitalRadiographyPACS2010}.

Unsharp masking is a simple method for edge enhancement and sharpening of images, and consist of
three core steps:

\begin{description}
    \item[Step 1] The image is blurred using low-pass filtering, e.g. via a Gaussian filter. The wider the kernel size,
    the more blurred the low-pass image is, and structures lower than the kernel size are almost completely suppressed.
    \item[Step 2] Subtracting the low-pass image from the original yields the high-pass information and contains those
    details that are suppressed on the low-pass image. If the kernel size is not too small, most sharp contours on the
    original image will be retained in the high-pass image.
    \item[Step 3] For the enhanced image, various weighted combinations of the original, low-pass or high-pass images
    can be created. A common approach is to add the weighted high-pass image to the original image, leading to an
    amplification of the detail information contained in it \cite[p.~152-153]{Prokop2003},.
\end{description}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/unsharp_mask.png}
    \caption{Unsharp masking algorithm \cite[Figure~5]{Prokop2003}}
    \label{fig:unsharp_masking}
\end{figure}

The effects of unsharp masking depend critically on the size of the filter kernel used to create the blurred image.
Kernels of 20-30mm typically give the best results for general chest imaging \cite[p.~153]{Prokop2003},
as they enhance structures of diagnostic interest while avoiding suppression of diagnostic information.

More complex algorithms such as Multiscale processing can apply the effect of unsharp masking on multiple scales
by splitting an image into many frequency bands, enabling multiple size-specific enhancements \cite[p.~156]{Prokop2003}.

\subsection{Transmission Maps}

Despite the processes behind capturing X-ray images are well studied, to our knowledge, there are currently no open datasets of X-ray
transmission maps. We had access to, precisely, two images taken from the same chest phantom at the Hard X-ray lab
in KTH \todo{is this the right source}, one paired with a processed image produced by a clinical-grade detector. Figure
\ref{fig:chestPhantomImages} shows the obtained sample pair of transmission map and processed image.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/comm_unprocessed_sample.jpg}
        \caption{Real transmission map.}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/comm_processed_sample.jpg}
        \caption{Processed X-ray image (clinical detector).}
    \end{subfigure}
    \caption{Sample pair of images from chest phantom}
    \label{fig:chestPhantomImages}
\end{figure}

For this thesis, a method is proposed based on the known physics of x-ray imaging, and insights that can be
taken from the analysis of the sample pair shown in \autoref{fig:chestPhantomImages}.

\begin{description}
    \item[Collimated area range] Only a fraction of the dynamic range of a transmission map
   contains diagnostic information. Figure \ref{fig:transmissionMapsHistograms} shows the histograms
   of the transmission map samples. It can be observed that the captured transmission values are concentrated in
  the range below 0.4, with the outliers corresponding to non-diagnostic relevant regions.
  \begin{figure}[H]
        \centering
        \includegraphics[width=1.0\textwidth]{figures/transmission_maps_histograms.png}
        \caption{Histograms of chest phantom transmission maps}
        \label{fig:transmissionMapsHistograms}
    \end{figure}

    \item[Body tissue groups by absorption coefficients] \autoref{sec:human_chest_imaging} presented the absorption coefficients
    for the different body tissues present in the chest. The coefficients for the different soft body tissues
    vary slightly, by about 2\% of the dynamic range of an X-ray measurement \cite[p.~54]{epstein2008}. On the other hand
    lung (air filled) and bone regions will have a significant difference in energy absorption. This can lead to
    the definition of three groups of body regions that will lead to similar transmission values, described in
    \autoref{tab:segmentationGroups}. It can be noted that these anatomical structures do not span the entirety of the
   chest. These are limited to structures we can identify via an existing segmentation model, as defined in \autoref{sec:segmentation}.

    \begin{table}[H]
        \centering
        \begin{tabular}{l l}
            \textbf{Group name} & \textbf{Grouped labels} \\
            \hline
            Bone &  Left Clavicle\\
                & Right Clavicle \\
                & Left Scapula\\
                &Right Scapula\\
                &Spine \\
            \hline
            Lung &  Left Lung \\
                & Right Lung \\
                & Left Hilus Pulmonis \\
                & Right Hilus Pulmonis \\
            \hline
            Soft tissue &  Heart \\
            & Aorta \\
            & Mediastinum \\
            & Facies Diaphragmatica \\
            & Weasand
        \end{tabular}
        \caption{Anatomical structures grouped by absorption coefficients}
        \label{tab:segmentationGroups}
    \end{table}

    \item[Histogram cluster ranges] Figure \ref{fig:segmentationTransmissionMapsHistograms} shows the histograms
    by different tissues (see \ref{sec:segmentation}). Each of the histograms, despite overlapping, all of them
    have defined boundaries.

    \begin{figure}[H]
        \centering
        \includegraphics[width=1.0\textwidth]{figures/segmentation_txm_histograms.png}
        \caption{Histograms of chest phantom transmission maps grouped by tissue groups}
        \label{fig:segmentationTransmissionMapsHistograms}
    \end{figure}

    \item[Non-flatten histogram] Figure \ref{fig:processedHistogramCompare} shows the histograms of a real pair of transmission map
    and processed image. There is an evident difference with the processed image having more uniform histograms,
    likely resulting from a histogram equalization process. In contrast, the transmission map histogram has higher
    density in the values closer to zero.
    \begin{figure}[H]
        \centering
        \includegraphics[width=1.0\textwidth]{figures/processed_hist_cmp.png}
        \caption{Histograms of processed transmission maps}
        \label{fig:processedHistogramCompare}
    \end{figure}
\end{description}


\chapter{Problem Formulation}

The design of the presented method is motivated by the following findings coming from prior knowledge on X-ray images
and the \acrshort{DIP} algorithms used in the field:

\begin{itemize}
    \item Despite differences in processing algorithms across vendors, all seek the same perceptual effects
    \cite[p.~119]{carterDigitalRadiographyPACS2010}, and iterations of such algorithms avoid changes in the appearance
    that radiologists expect from X-ray images \cite[p.~57]{STA00a}.
    \item If we recover an appropriate forward model (image processing algorithm) that can produce images perceptually
    similar to the target X-ray images, we may use it as a basis to reconstruct 'realistic' transmission maps from
    the processed images.
\end{itemize}

Moreover, there is little to none prior information about transmission maps. Thus, if it's not possible to
design a model constrained on data, it might be possible to constrain it on the transformations that map the
latent images to the observed X-ray datasets.

\section{Mathematical Model}
\label{sec:mathematicalModel}

For a dataset of observed, grayscale processed X-ray images $Y = \{y_i\}^N_{i=1}$, we aim to recover a corresponding
set of transmission maps $X = \{x_i\}^N_{i=1}$, with $x_i, y_i \in [0, 1]^{r, c}$ that satisfy
\begin{equation}
    F(x_i, \theta) = y_i.
\end{equation}

Here, $F:[0,1]^{r, c} \times \mathbb{R}^d \to [0, 1]^{r, c}$ denotes a known image processing algorithm governed by
parameters $\theta \in \mathbb{R}^d$.

This can be formulated as an optimization problem: for each image $y_i$, we aim to find the transmission map $x_i$
and parameters $\theta_i$ that minimize a loss function $\ell(x_i, \theta_i)$:

\begin{equation}
    \label{eq:optimizationProblem}
    \min_{x_i, \theta_i \in \Omega_\theta}  \underbrace{\mathcal{D}(y_i, F(x_i, \theta_i)) + \sum_{j = 1}^{K} \lambda_k R_j(x_i)}_{=: \ell(\cdot)}.
\end{equation}

Here, $\mathcal{D}$ is a data fidelity function measuring the discrepancy between the processed versions of
our estimated transmission maps and the observed processed images, and $R_1, \dots R_K$ are regularization
terms on $x_i$ weighted by $\lambda_1, \dots, \lambda_K$.

As $F$ is chosen based on existing \acrshort{DIP} algorithms, we can therefore constraint the parameters $\theta_i$
on expected value ranges and values recommended in the literature. Thus, the problem is further constrained by finding
parameters $\theta_i$ that lie in the feasible ranges

\begin{equation}
    \theta_i \in \Omega_\theta = \left\{ \theta_i \mid c_j(\theta_i) \geq 0, j \in \mathcal{I}  \right\}.
\end{equation}

The presented model suggests that each image $y_i$ will have a corresponding transmission map $x_i$ and
parameters $\theta_i$. However, the following special cases of the problem can also be considered:

\begin{description}
    \item[Common operator parameters] If we assume that there exists a single set of parameters $\theta$
    (yet unknown) for all processed images, i.e. $F(x_i, \theta) = y_i$ for all $i = \{1,\dots,N\}$.
    Then the problem can be modeled as
    \begin{equation}
        \min_{x_i, \theta_0 \in \Omega_\theta}  \mathcal{D}(y_i, F(x_i, \theta_0)) + \sum_{j = 1}^{K} \lambda_k R_j(x_i)
    \end{equation}
    \item[Known parameters] If we assume there is a known set of parameters $\theta_0$ for the operator
    $F$ that can model all the observed processed images, then we reduce the problem to
    \begin{equation}
        \min_{x_i}  \mathcal{D}(y_i, F(x_i, \theta_0)) + \sum_{j = 1}^{K} \lambda_k R_j(x_i).
    \end{equation}
\end{description}


The method employed in this project involved an iterative process of defining a data fidelity function and
regularization functions that will recover transmission maps considered realistic (according to a predefined
set of characteristics). In the following sections, we will discuss the choices made for
$F$, $\mathcal{D}$ and $R_j$, and their effects on the reconstruction of transmission maps.



\subsection{Segmentation}
\label{sec:segmentation}

One of the core validation measures for the recovered transmission maps is the expected transmitted x-rays on
different body tissues. As presented in \autoref{tab:segmentationGroups}, three groups of tissues
can be defined that will have similar transmission values, and differ significantly across them. This
will drive the choice of a regularization term based on the expected transmission values relative to
the location of a pixel.

To identify these groups, the ChestXDet \cite{chestxdet} model was used to obtain segmentation labels.
Concretely, the segmentation model identifies 13 labels, which are grouped in 3 major categories.
These groups and their corresponding labels are listed in Table \ref{tab:segmentationGroups}.

\subsubsection{Mask groups}

To compute the segmentation masks, the ChestXdet model returns confidence values on a 0 to 1 range.
To get binary masks, we include in our model a threshold parameter. The threshold is used to create binary masks where
the values are above it, then the segmentation targets are joined according to the groups described in
\autoref{tab:segmentationGroups}. The join operation is performed by taking the logical
OR of the masks.

Since the mask groups may contain overlapping regions, a difference is applied to obtain exclusive masks. This is done in an ordered manner,
starting from the groups with higher absorption (bone) up to lower absorption values (lung). This ensures that each pixel is assigned to
the mask that produces the higher attenuation. The complete merging operation is described in algorithm \ref{alg:mergeSegmentationMasks}. Figure
\ref{fig:segmentationMasks} shows a sample set of the processed segmentation masks over a CheXpert image using a threshold of 0.5.

\input{algorithms/merge_segmentation_masks.tex}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/segmentation_masks.png}
    \caption{Segmentation masks}
    \label{fig:segmentationMasks}
\end{figure}

\section{Forward Operator Model}

Throughout the entire model experimentation, a fixed forward operator is used to represent a
digital radiography processing pipeline. The goal when building an operator is to work with a
model that can produce X-ray images with an appearance consistent to the existing image datasets.
Then, there is the expectation that inverting this would produce transmission maps with realistic
values.

The choices of the steps for the operator are driven by the goals of image processing described
in \autoref{sec:DigitalImageProcessing}. These guide the design of our operator $F$ as a composition
of several transformations

\begin{equation}
F = F_n \circ F_{n-1} \circ \cdots \circ F_1
\end{equation}

that include:
\begin{description}

\item[Negative logarithm] $F_1(x) = -\log(x + \epsilon)$, where $\epsilon$ is a small constant to avoid numerical instability

    Transmission maps represent the ratio of X-ray intensities $I_1$ and $I_0$,

    \begin{equation}
        \frac{I_1}{I_0} = e^{-\mu x}.
    \end{equation}

    However, processed images operate on the thickness terms $\mu x$
    (explaining the negative relationship between transmission maps and diagnostic images),
    which can be extracted through a negative logarithm.

\item[Windowing] As a way of implementing gradational adjustment, a window function is implemented, that creates
an S-shape lookup table to achieve signal normalization:

\begin{equation}
    F_2(x) = \frac{1}{1+e^{-\gamma \frac{x - c}{w}}},
\end{equation}

where $c$ is the center of the sigmoid function, $w$ is a width parameter, and $\gamma$ is a steepness parameter.
The effect of these parameters and how these translate into a LUT is shown in \autoref{fig:windowingParams}.
By definition, $c$ and $w$ are restricted to the domain $(0, 1)$, thus introduce box constraints on
$\theta$. For $gamma$.

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/window_params.png}
    \caption{Windowing function with different parameter settings.}
    \label{fig:windowingParams}
\end{figure}

\item[Unsharp masking]
\begin{equation}
    F_3(x) = \frac{x - \alpha \cdot G_\sigma * x}{1.0 - \alpha}
\end{equation}

where $G_\sigma$ is a Gaussian kernel with standard deviation $\sigma$, and $\alpha$ is the enhance factor.
The chosen formulation bounds $\alpha$ to $(0, 1)$. However, since it is known that images go through
some edge enhancement, the lower bound is increased to 0.1. Regarding the frequency band represented by
$\sigma$, the implementation considers a fixed value, or a range that will capture higher-frequencies,
following the recommendation of targeting a 20-30mm kernel size \cite[p.~153]{Prokop2003}.

\item[Range normalization]
\begin{equation}
    F_4(x) = \frac{x - \min(x)}{\max(x) - \min(x)},
\end{equation}
as a mean to normalize values to the $[0,1]$ range, since operations such as windowing and unsharp masking can lead
to values outside this range.

\item[Clipping]
\begin{equation}
    F_5(x) = \min(\max(x, 0), 1)
\end{equation}
\end{description}

The parameter vector $\theta$ includes all parameters of these transformations, such as $\{c, w, \alpha, \sigma\}$.

\section{Optimization Approach}
\label{sec:optimization}

As depicted in \autoref{eq:optimizationProblem}, the transmission map recovery involves solving a constrained
non-linear optimization problem. The approach taken considers Gradient Descent methods and, for the case of
the $\theta$ inequality constraints, projections onto the feasible set. However, the ill-posedness nature of our
problem prevents the choice of an ideal optimization method (i.e. step size selection), and hyperparameters
analytically. Thus, experiments are conducted to find an appropriate loss function and a set of hyperparameters
that consistently converges to 'valid' solutions.

For a recovered transmission map $x_i$, the concept of a 'valid' solution is defined by meeting the following criteria:

\begin{enumerate}
    \item $F(x_i, \theta)$ is a close approximation to the observed image $y_i$.
    \item $x_i$ does not contain artifacts or noise.
    \item $x_i$ has suppressed high-frequency components, in contrast to $y_i$.
    \item The transmission values of $x_i$ at the three anatomical groups described in \autoref{tab:segmentationGroups}
    are within their expected range.
\end{enumerate}

\subsubsection{Data Fidelity Term}

The data fidelity term is a measure of how close the reconstructed image $F(x_i, \theta)$ is to the original image $y_i$.
For the proposed model, two alternatives are considered. The first one is \acrlong{MSE} (\acrshort{MSE}),

\begin{equation}
    \mathcal{D}_1(x, y) = \frac{1}{2} \lVert x - y \rVert_2^2,
\end{equation}

which has been a popular choice of cost function in image restoration problems, due to its ability to favor a
high \acrlong{PSNR} (\acrshort{PSNR}), widely used metric for measuring image restoration quality \cite[p.~1]{zhaoLossFunctionsImage2017}, \cite[p.~191]{fleetComputerVisionECCV2014}.
However, this metric has been shown to not correlate well with human’s perception of image quality,
and can yield the same quantity over distortions on different scales of the image \cite[Figure~2]{wangMultiscaleStructuralSimilarity2003}.
For our model, accuracy in the image reconstruction is critical due to the expected operations in the high frequencies
of the image.

Therefore, an alternative is a loss metric proposed by \cite{zhaoLossFunctionsImage2017} that combines
\acrfull{MS-SSIM} with the $\ell_1$ loss (Mean Absolute Error). \acrshort{SSIM} is an Image Quality Assessment
metric that evaluates images accounting for the fact that the \acrfull{HVS} is sensitive to changes in local
structure. \acrshort{MS-SSIM} was later proposed to extend \acrshort{SSIM} to account for the fact that the
scale at which local structure should be analyzed depends on the distance to the observer. This extension
is weights \acrshort{SSIM} computed at different scales. Despite its demonstrated good performance in image reconstruction
problems, outperforming other loss functions, it is significantly more computationally expensive, which
incentivates the alternative choice of \acrshort{MSE}.

\subsection{Regularization}

The selection of regularization terms is guided by the soft constraints expected from the reconstructed transmission
maps. These have the goals of reducing noise and artifacts from the images, as well as promoting the
compliance with the expected transmission ranges at each of the absorption-similar regions.

\begin{description}
    \item[Total Variation] TV regularization promotes piecewise smoothness in $x_i$, thus discouraging discontinuities
    in the image. Despite the fact that TV regularization can lead to over-smoothed images, it is still explored due to
    its low computational cost.
    \item[Segmentation-based regularization] We incorporate physics-based constraints derived from anatomical segmentation
    to ensure realistic transmission values for each tissue type. For each of the anatomical groups discussed in
   \autoref{sec:segmentation}, a value range is extracted from a real transmission map, and is used as ideal bounds for
   recovered transmission. The formulation of this regularization consists of a square penalty for values outside their
   defined bounds (if any). Let $s_i^{t}(x, y)$ denote the binary mask for $x_i$ of the anatomical group $t$,
   and $v^{t}_{min}, v^{t}_{max}$ be the expected transmission bounds for said group. Then,

   \begin{equation}
   p_t(x_i, s^t_i) = \sum_{j, k}  s_i^t(j, k) \cdot \left[ \max(0, x_i(j, k) - v^{t}_{max})^2 + \max(0, v^{t}_{min} - x_i(j, k) )^2\right]
   \end{equation}

   is the penalty on $x_i$ that is zero when all the pixels within the mask $s_i^t$ are within their respective bounds
   and increases quadratically as the values deviate from them. This penalty is always introduced in the loss function
   for all explorations.
\end{description}


\subsection{Solution}

For finding the solution of the minimization problem, we recurr to iterative descent methods. These refer to
iterative algorithms that compute a sequence of parameters $x^{(k)}$, $\theta^{(k)}$ that will reduce the
loss function at each iteration, i.e.

\begin{equation}
    \ell(x^{(k)}, \theta^{(k)}) <  \ell(x^{(k - 1)}, \theta^{(k - 1)}), \qquad \text{for all } k > 0.
\end{equation}

The general descent method can be described as follows:

\begin{enumerate}
    \item $k := 0$
    \item Choose initial paramaters $x^{(0)}$, $\theta^{(0)}$.
    \item If $k > 0$ and $\ell(x^{(k)}, \theta^{(k)}) - \ell(x^{(k - 1)}, \theta^{(k - 1)}) < \epsilon$, stop.
    \item If $k > max\_iterations$, stop.
    \item Compute descent directions $d_x^{(k)}$, $d_\theta^{(k)}$
    \item Compute step sizes $\sigma_x^{(k)}$, $\sigma_\theta^{(k)}$
    \item $x^{(k + 1)} = x^{(k)} + \sigma_x^{(k)} d_x^{(k)}$
    \item $\theta^{(k + 1)} = \mathrm{proj}_{\Omega_\theta}(\theta^{(k)} + \sigma_\theta^{(k)} d_\theta^{(k)}$)
    \item Go to 3.
\end{enumerate}

The choice of descent direction is typically the negative gradient, since it points in the direction of the steepest
decrease. However, the choice of a step sizes is non trivial, and we experiment with multiple known gradient descent
optimizers to compute updates.

Regarding the constraints on the $\theta$ parameters, we recurr to a projected gradient descent method, where
each update of  $\theta_k$ is first projected into the admissible domain via

\begin{equation}
    \mathrm{proj}_{\Omega_\theta}(\theta_0) = \arg\min_{\theta \in \Omega_\theta} \lVert \theta - \theta_0 \rVert_2.
\end{equation}

Since all the specified constraints are box constraints, the minimization is satisfied when each parameter is
clipped to its respective bounds.

\section{Evaluation Methodology}

Due to the lack of ground truth, we rely on qualitative assessments and visual evaluations based on our
validity criteria. That translates into finding a model that produces solutions that: 1) reproduce estimated processed
images close to the original images, on multiple scales, and 2) satisfy the set physical constraints.

For the large image datasets, we compute image quality metrics such as SSIM and PSNR between reconstructed processed
images and originals, and measure an error on the transmission values that are outside the restricted physical bounds.
For the limited cases where ground truth is available (e.g., from phantoms), we also use the forward model to generate
alternative processed images, as well as the original paired processed images. That allows us to compare the
performance of our model on ground truth.

\chapter{Implementation}

The implementation of the proposed model consisted on building a forward operator that can produce images with a similar
visual appearance to images found in datasets such as CheXpert and ChestX-ray8. As for the optimization task,
\textit{JAX} \cite{jax2018github} and \textit{Optax} \cite{deepmind2020jax} are used to implement the
algorithm described in \autoref{sec:optimization}. JAX is a Python array computation library that provides automatic
differentiation, handling the computation of gradients for the chosen loss function, and consequently, the processing
operator. \textit{Optax} is an gradient processing and optimization library that provides multiple gradient descent optimizers, ruled
by additional hyperparameters, such as a learning rate.

Since the approach taken for this project does not involve learning an inverse model, but rather solve an optimization
on a per-image basis, we aim to find a model and corresponding hyperparameters that can be reused across different
sets of images and guarantee convergence into valid solutions. Besides optimization-related hyperparameters such as
learning rates, we also consider different choices that can produce valid solutions but won't match qualitatively
the expectations for transmission maps. One such choice is a regularization term (besides its corresponding weight)
that promotes smoothness on the transmission map, which if not chosen properly, can lead to artifacts or over-smoothing.
Overall, the hyperparameters and configurations to find are listed in \autoref{tab:hyperparametersList}.

\begin{table}
    \centering
    \begin{tabular}{|c|l{0.3\linewidth}|}
        \hline
        Hyperparameter & Description \\
        \hline
        Optimizer & Choice of optimizer, e.g. Adam, SGD, RMSProp. \\
        \hline
        Optimizer hyperparameters & Optimizer-specific, such as learning rate, momentum, weight decay, etc. \\
        \hline
        Segmentation threshold & Threshold to delimit binary segmentation masks, set to 0.5 for all results. \\
        \hline
        Frequency bands & Usharp mask is an operation that amplifies specific frequency bands, which are determined
        by the kernel size for the blurring operation. The processing model assumes enhancement on at least two
        frequency bands, low and high. The consequence of this is the exclusion of this parameter from the optimization
        problem, but it is rather fixed for all images.\\
        \hline
        Regularization weights & Weights for the smoothing and segmentation regularization. \\
        \hline
        Smoothing regularization & The choice among Tikhonov, Total Variation, and no regularization, to penalize discontinuities in the transmission maps. \\
        \hline
        Max steps & Maximum number of iterations for the optimization. \\
        \hline
        Max steps & Maximum number of iterations for the optimization. \\
        \hline
    \end{tabular}
    \label{tab:hyperparametersList}
    \caption{Model hyperparameters}
\end{table}

For finding hyperparameters, a Bayesian search was done, optimizing for the perceptual metric MS-SSIM.
An optimal choice of parameters can later be found by picking the set that also satisfies the
segmentation constraints. To implement this search, we used Weights \& Biases \cite{wandb} sweep
functionality.

\section{Data Collection and Preprocessing}
\label{sec:dataCollection}

Throughout the experimentation, besides the single transmission map-processed image pair, images were sampled from
cheXpert dataset \cite{chexpert} and NIH Chest X-ray dataset \cite{nih}. Recall the motivation to use these datasets
is to get value from the diagnostic labels they provide, which can then be tied to the simulated transmission maps.
However, these labels are not used for the model.

Although we present an optimization problem that operates per-image, it is expected that multiple images will
be processed in parallel to speed up the optimization process. Thus, a fixed image size is set for the implementation.
Images from the datasets also do not have a standard size or aspect ratio, so all images are preprocessed by
cropping by the center to keep a square aspect ratio and resized to 512 pixels width, as well as rescaling the
grayscale values to the range [0, 1]. The choice of the size 512 is set to match the input size of the selected
segmentation model implemented in torchxrayvision \cite{torchxrayvision}, however, an arbitrary size can be chosen
by upscaling the masks. The full data preprocessing steps are shown in \autoref{fig:preprocessing_steps}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/data_loading.png}
    \caption{Data preprocessing pipeline}
    \label{fig:preprocessing_steps}
\end{figure}

\section{Experimental Setup}
Our experiments are configured to explore different models and hyperparameters:

\begin{itemize}
    \item \textbf{Known vs. Unknown Transformation}: We compare scenarios where the forward transformation is known versus when it must be inferred
    \item \textbf{Single vs. Batch Processing}: We evaluate optimizing for individual images versus batches
    \item \textbf{Regularization Strength}: We vary the total variation regularization parameter
    \item \textbf{Initialization Strategies}: We test different initialization methods for the transmission maps
\end{itemize}

Each experiment is run with multiple random seeds to ensure robustness, and results are logged to Weights \& Biases for analysis.
\chapter{Results and Analysis}
\label{ch:resultsAndAnalysis}

In this chapter, we present the experimental results of our transmission map recovery method and analyze its performance across different configurations and datasets.

\section{Experimental Setup}
Our experiments were conducted on subsets of the CheXpert dataset, with the following configurations:
\begin{itemize}
    \item \textbf{Batch sizes}: 8, 16, and 32 images
    \item \textbf{Image resolution}: 512×512 pixels (downsampled from original)
    \item \textbf{Optimization steps}: 300, 600, and 1200 iterations
    \item \textbf{Hardware}: NVIDIA RTX 3090 GPU with 24GB memory
\end{itemize}

\section{Hyperparameter Optimization Results}
Through Bayesian optimization over 200 sweep runs, we identified optimal hyperparameter ranges:

\begin{table}[ht]
\centering
\caption{Optimal hyperparameter ranges identified through Bayesian optimization}
\label{tab:hyperparameters}
\begin{tabular}{l|c|c}
\hline
\textbf{Parameter} & \textbf{Optimal Range} & \textbf{Best Value} \\
\hline
Learning rate & 0.005 - 0.04 & 0.0084 \\
Gradient penalty weight ($\lambda_{\text{grad}}$) & 0 - 0.0001 & 9.8e-6 \\
Anatomical prior weight ($\lambda_{\text{anat}}$) & 0.1 - 0.5 & 0.261 \\
Detail similarity weight ($\lambda_{\text{detail}}$) & 0.1 - 0.45 & 0.384 \\
\hline
\end{tabular}
\end{table}

\section{Reconstruction Quality Metrics}
[Note: Insert actual results here once experiments are complete]

\subsection{Image Quality Assessment}
The reconstruction quality was evaluated using:
\begin{itemize}
    \item \textbf{SSIM}: Structural similarity between reconstructed processed images and targets
    \item \textbf{PSNR}: Peak signal-to-noise ratio
    \item \textbf{Anatomical Compliance}: Percentage of pixels within expected physical ranges for each tissue type
\end{itemize}

\subsection{Physical Plausibility}
We assess the physical plausibility of recovered transmission maps through:
\begin{itemize}
    \item Distribution analysis of transmission values per anatomical region
    \item Comparison with the limited ground truth phantom data
    \item Expert radiologist evaluation (planned)
\end{itemize}

\section{Computational Performance}
\subsection{Scaling Analysis}
The computational requirements scale as:
\begin{itemize}
    \item \textbf{Memory}: $O(B \times H \times W)$ where $B$ is batch size
    \item \textbf{Time}: Approximately linear in number of optimization steps
    \item \textbf{Convergence}: Typically achieved within 300-600 iterations for well-chosen hyperparameters
\end{itemize}

\subsection{Bottlenecks and Limitations}
The primary computational bottlenecks identified are:
\begin{enumerate}
    \item Segmentation model inference (can be mitigated through pre-computation)
    \item Gaussian blur operations in unsharp masking (limited by convolution efficiency)
    \item Hyperparameter search space exploration (requires extensive parallel computation)
\end{enumerate}

\cleardoublepage
\chapter{Discussion}
\label{ch:discussion}

\section{Interpretation of Results}
Our method demonstrates the feasibility of recovering physically plausible transmission maps from processed chest X-rays, though several challenges remain for practical deployment at scale.

\subsection{Success Factors}
The combination of gradient-based smoothness regularization with anatomically-informed constraints proves effective in producing transmission maps that:
\begin{itemize}
    \item Maintain appropriate value ranges for different tissue types
    \item Preserve diagnostic features when re-processed
    \item Exhibit smooth transitions consistent with physical X-ray attenuation
\end{itemize}

\subsection{Limitations and Challenges}
\subsubsection{Scalability}
The current approach faces significant scalability challenges:
\begin{itemize}
    \item \textbf{Computational Cost}: Processing the full CheXpert dataset would require approximately 100-400 GPU-hours depending on batch size and convergence criteria
    \item \textbf{Hyperparameter Sensitivity}: Optimal parameters vary with image characteristics, requiring adaptive strategies
    \item \textbf{Memory Constraints}: Large batch sizes improve parameter estimation but are limited by GPU memory
\end{itemize}

\subsubsection{Model Assumptions}
Our forward model makes several simplifying assumptions:
\begin{itemize}
    \item Single processing pipeline for all manufacturers (may not capture vendor-specific variations)
    \item Fixed order of operations (actual pipelines may vary)
    \item Deterministic processing (ignores potential adaptive or ML-based enhancements)
\end{itemize}

\section{Comparison with Alternative Approaches}
Unlike direct inverse mapping approaches, our optimization-based method:
\begin{itemize}
    \item Provides interpretable parameters for the forward model
    \item Allows incorporation of physical constraints
    \item Can adapt to different processing pipelines through parameter adjustment
\end{itemize}

However, it requires significantly more computation than learned inverse mappings.

\section{Practical Implications}
For practical deployment, we recommend:
\begin{enumerate}
    \item \textbf{Hierarchical Processing}: Start with low-resolution optimization to identify good initialization points
    \item \textbf{Transfer Learning}: Use parameters learned from one batch to initialize nearby batches
    \item \textbf{Selective Processing}: Focus on high-value images (e.g., those with specific pathologies)
\end{enumerate}

\cleardoublepage
\chapter{Conclusions and Future work}
\label{ch:conclusionsAndFutureWork}

\section{Conclusions}
\label{sec:conclusions}

This thesis presents a novel approach to recovering X-ray transmission maps from processed chest radiographs through constrained optimization. Our key contributions include:

\begin{enumerate}
    \item \textbf{Differentiable Forward Model}: We developed a fully differentiable pipeline modeling common radiographic processing steps, enabling gradient-based optimization.

    \item \textbf{Anatomically-Informed Regularization}: By incorporating segmentation-guided constraints, we ensure recovered transmission maps respect the physical properties of different tissue types.

    \item \textbf{Multi-Objective Optimization Framework}: Our approach balances data fidelity, spatial smoothness, and perceptual quality through carefully designed loss functions.
\end{enumerate}

The method successfully recovers transmission maps that, when processed through our forward model, closely match the target clinical images while maintaining physical plausibility.

\section{Limitations}
\label{sec:limitations}

Several limitations constrain the current approach:

\begin{itemize}
    \item \textbf{Computational Scalability}: Processing large datasets remains computationally intensive, requiring days of GPU time for comprehensive datasets.

    \item \textbf{Hyperparameter Sensitivity}: Optimal parameters vary across image characteristics, necessitating extensive search or adaptive strategies.

    \item \textbf{Ground Truth Scarcity}: Limited availability of paired transmission map and processed image data restricts validation options.

    \item \textbf{Model Complexity}: The simplified forward model may not capture all nuances of commercial processing pipelines.
\end{itemize}

\section{Future work}
\label{sec:futureWork}

\subsection{Immediate Extensions}
\begin{enumerate}
    \item \textbf{Adaptive Hyperparameter Selection}: Develop methods to automatically select hyperparameters based on image characteristics, potentially using meta-learning approaches.

    \item \textbf{Efficient Optimization Strategies}: Explore second-order optimization methods or learned optimizers to reduce the number of iterations required.

    \item \textbf{Multi-Resolution Processing}: Implement coarse-to-fine optimization strategies to improve both speed and convergence.
\end{enumerate}

\subsection{Long-term Research Directions}
\begin{enumerate}
    \item \textbf{Learned Inverse Models}: Train neural networks to directly predict transmission maps, using our optimization results as training data.

    \item \textbf{Manufacturer-Specific Models}: Develop specialized forward models for different equipment manufacturers based on their processing characteristics.

    \item \textbf{Uncertainty Quantification}: Incorporate Bayesian approaches to quantify uncertainty in recovered transmission maps.

    \item \textbf{Real-time Processing}: Develop approximation methods enabling real-time transmission map recovery for clinical applications.
\end{enumerate}

\subsection{Practical Deployment}
For practical deployment in support of low-cost X-ray system development:
\begin{itemize}
    \item Create a cloud-based processing pipeline for batch transmission map recovery
    \item Develop quality metrics to automatically identify successfully recovered maps
    \item Build a curated dataset of transmission maps with associated metadata
    \item Establish validation protocols with clinical partners
\end{itemize}

\section{Reflections}
\label{sec:reflections}

This work contributes to global health equity by enabling the development of affordable diagnostic imaging systems. The ability to generate training data for alternative X-ray detectors could significantly reduce the cost barrier for medical imaging in resource-limited settings.

The project also highlights the importance of physics-informed machine learning approaches in medical imaging, demonstrating how domain knowledge can guide optimization in under-constrained problems.

While significant computational challenges remain, the framework established here provides a foundation for future work in radiographic image analysis and affordable medical technology development.


One of the most important results is the reduction in the amount of
energy required to process each packet while at the same time reducing the
time required to process each packet.

The thesis contributes to the \gls{UN}\enspace\glspl{SDG} numbers 1 and 9 by
xxxx.




\noindent\rule{\textwidth}{0.4mm}
\engExpl{In the references, let Zotero or other tool fill this in for you. I suggest an extended version of the IEEE style, to include URLs, DOIs, ISBNs, etc., to make it easier for your reader to find them. This will make life easier for your opponents and examiner. \\IEEE Editorial Style Manual: \url{https://www.ieee.org/content/dam/ieee-org/ieee/web/org/conferences/style_references_manual.pdf}}
\sweExpl{Låt Zotero eller annat verktyg fylla i det här för dig. Jag föreslår en utökad version av IEEE stil - att inkludera webbadresser, DOI, ISBN osv. - för att göra det lättare för läsaren att hitta dem. Detta kommer att göra livet lättare för dina opponenter och examinator.}

\cleardoublepage
% Print the bibliography (and make it appear in the table of contents)
\renewcommand{\bibname}{References}


\ifbiblatex
    %\typeout{Biblatex current language is \currentlang}
    \printbibliography[heading=bibintoc]
\else
    \phantomsection  % make it include a hyperref - see https://tex.stackexchange.com/a/98995
    \addcontentsline{toc}{chapter}{References}
    \bibliography{references}
\fi



\warningExpl{If you do not have an appendix, do not include the \textbackslash cleardoublepage command below; otherwise, the last page number in the metadata will be one too large.}
\cleardoublepage
\appendix
\renewcommand{\chaptermark}[1]{\markboth{Appendix \thechapter\relax:\thinspace\relax#1}{}}
\chapter{Supporting materials}
\label{sec:supportingMaterial}
\generalExpl{Here is a place to add supporting material that can help others build upon your work. You can include files as attachments to the PDF file or indirectly via URLs. Alternatively, consider adding supporting material uploaded as separate files in DiVA.}

% Attach the BibTeX for your references to make it easy for a reader to find and use them
The BibTeX references used in this thesis are attached. \attachfile[description={references.bib}]{references.bib}

% Attach source code file(s) or add a URL to the github or other repository
Some source code relevant to this project can be found at \url{https://github.com/gqmaguirejr/E-learning} and \url{https://github.com/gqmaguirejr/Canvas-tools}.

Your reader can access the attached (embedded) files using a PDF tool such as Adobe Acrobat Reader using the paperclip icon in the left menu, as shown in \Cref{fig:PDFreaderPaperclipExample} or by right-clicking on the push-pin icon in the PDF file and then using the menu to save the embedded file as shown in \Cref{fig:PDFreaderPushpinExample}.

An argument for including supporting material in the PDF file is that it will be available to anyone who has a copy of the PDF file. As a result, they do not have to look elsewhere for this material. This comes at the cost of a larger PDF file. However, the embedded files are encoded into a compressed stream within the PDF file; thus, reducing the number of additional bytes. For example, the references.bib file that was used in this example is \SI{10617}{\byte} in size but only occupies \SI{4261}{\byte} in the PDF file.

\warningExpl{DiVA is limited to $\approx$\SI{1}{\giga\byte} for each supporting file. If you have very large amounts of supporting material, you will probably want to use one of the data repositories. For additional help with this, contact KTH Library via
\href{mailto:researchdata@kth.se}{researchdata@kth.se}.\\As of Spring 2024, there are plans to migrate this supporting data from DiVA to a research data repository.
}

\begin{figure}[!ht]
  \begin{center}
    \includegraphics[width=0.50\textwidth]{README_notes/pdf-viewer-attached-files.png}
  \end{center}
  \caption{Adobe Acrobat Reader using the paperclip icon for the attached references.bib file}
  \label{fig:PDFreaderPaperclipExample}
\end{figure}
\FloatBarrier

\begin{figure}[!ht]
  \begin{center}
    \includegraphics[width=0.99\textwidth]{README_notes/Bib-save-embedded-example.png}
  \end{center}
  \caption{Adobe Acrobat Reader after right-clicking on the push-pin icon for the attached references.bib file}
  \label{fig:PDFreaderPushpinExample}
\end{figure}
\FloatBarrier
\cleardoublepage

\chapter{Something Extra}
\sweExpl{svensk: Extra Material som Bilaga}

\section{Just for testing KTH colors}
\ifdigitaloutput
    \textbf{You have selected to optimize for digital output}
\else
    \textbf{You have selected to optimize for print output}
\fi
\begin{itemize}[noitemsep]
    \item Primary color
    \begin{itemize}
    \item \textcolor{kth-blue}{kth-blue \ifdigitaloutput
    actually Deep sea
    \fi} {\color{kth-blue} \rule{0.3\linewidth}{1mm} }\\

    \item \textcolor{kth-blue80}{kth-blue80} {\color{kth-blue80} \rule{0.3\linewidth}{1mm} }\\
\end{itemize}

\item  Secondary colors
\begin{itemize}[noitemsep]
    \item \textcolor{kth-lightblue}{kth-lightblue \ifdigitaloutput
    actually Stratosphere
    \fi} {\color{kth-lightblue} \rule{0.3\linewidth}{1mm} }\\

    \item \textcolor{kth-lightred}{kth-lightred \ifdigitaloutput
    actually Fluorescence\fi} {\color{kth-lightred} \rule{0.3\linewidth}{1mm} }\\

    \item \textcolor{kth-lightred80}{kth-lightred80} {\color{kth-lightred80} \rule{0.3\linewidth}{1mm} }\\

    \item \textcolor{kth-lightgreen}{kth-lightgreen \ifdigitaloutput
    actually Front-lawn\fi} {\color{kth-lightgreen} \rule{0.3\linewidth}{1mm} }\\

    \item \textcolor{kth-coolgray}{kth-coolgray \ifdigitaloutput
    actually Office\fi} {\color{kth-coolgray} \rule{0.3\linewidth}{1mm} }\\

    \item \textcolor{kth-coolgray80}{kth-coolgray80} {\color{kth-coolgray80} \rule{0.3\linewidth}{1mm} }
\end{itemize}
\end{itemize}

\textcolor{black}{black} {\color{black} \rule{\linewidth}{1mm} }

% Include an example of using nomenclature
\ifnomenclature
    \cleardoublepage
    \chapter{Main equations}
    \label{ch:NomenclatureExamples}
    This appendix gives some examples of equations that are used throughout this thesis.
    \section{A simple example}
    The following example is adapted from Figure 1 of the documentation for the package nomencl (\url{https://ctan.org/pkg/nomencl}).
    \begin{equation}\label{eq:mainEq}
    a=\frac{N}{A}
    \end{equation}
    \nomenclature{$a$}{The number of angels per unit area\nomrefeq}%       %% include the equation number in the list
    \nomenclature{$N$}{The number of angels per needle point\nomrefpage}%  %% include the page number in the list
    \nomenclature{$A$}{The area of the needle point}%
    The equation $\sigma = m a$%
    \nomenclature{$\sigma$}{The total mass of angels per unit area\nomrefeqpage}%
    \nomenclature{$m$}{The mass of one angel}
follows easily from \Cref{eq:mainEq}.

    \section{An even simpler example}
    The formula for the diameter of a circle is shown in \Cref{eq:secondEq} area of a circle in \cref{eq:thirdEq}.
    \begin{equation}\label{eq:secondEq}
    D_{circle}=2\pi r
    \end{equation}
    \nomenclature{$D_{circle}$}{The diameter of a circle\nomrefeqpage}%
    \nomenclature{$r$}{The radius of a circle\nomrefeqpage}%

    \begin{equation}\label{eq:thirdEq}
    A_{circle}=\pi r^2
    \end{equation}
    \nomenclature{$A_{circle}$}{The area of a circle\nomrefeqpage}%

    Some more text that refers to \eqref{eq:thirdEq}.
\fi  %% end of nomenclature example

\cleardoublepage
% Information for authors
%\include{README_author}
\subfile{README_author}

\cleardoublepage
% information about the template for everyone
\input{README_notes/README_notes}

\begin{comment}
% information for examiners
\ifxeorlua
\cleardoublepage
\input{README_notes/README_examiner_notes}
\fi
\end{comment}

\begin{comment}
% Information for administrators
\ifxeorlua
\cleardoublepage
\input{README_notes/README_for_administrators.tex}
\fi
\end{comment}

\begin{comment}
% Information for Course Coordinators
\ifxeorlua
\cleardoublepage
\input{README_notes/README_for_course_coordinators}
\fi
\end{comment}

%% The following label is necessary for computing the last page number of the body of the report to include in the "For DIVA" information
\label{pg:lastPageofMainmatter}

\cleardoublepage
\clearpage\thispagestyle{empty}\mbox{} % empty page with backcover on the other side
\kthbackcover
\fancyhead{}  % Do not use header on this extra page or pages
\section*{€€€€ For DIVA €€€€}
\lstset{numbers=none} %% remove any list line numbering
\divainfo{pg:lastPageofPreface}{pg:lastPageofMainmatter}

% If there is an acronyms.tex file,
% add it to the end of the For DIVA information
% so that it can be used with the abstracts
% Note that the option "nolol" stops it from being listed in the List of Listings

% The following bit of ugliness is because of the problems PDFLaTeX has handling a non-breaking hyphen
% unless it is converted to UTF-8 encoding.
% If you do not use such characters in your acronyms, this could be simplified.
\ifxeorlua
\IfFileExists{lib/acronyms.tex}{
\section*{acronyms.tex}
\lstinputlisting[language={[LaTeX]TeX}, nolol, basicstyle=\ttfamily\color{black},
commentstyle=\color{black}, backgroundcolor=\color{white}]{lib/acronyms.tex}
}
{}
\else
\IfFileExists{lib/acronyms-for-pdflatex.tex}{
\section*{acronyms.tex}
\lstinputlisting[language={[LaTeX]TeX}, nolol, basicstyle=\ttfamily\color{black},
commentstyle=\color{black}, backgroundcolor=\color{white}]{lib/acronyms-for-pdflatex.tex}
}
{}
\fi


\end{document}
